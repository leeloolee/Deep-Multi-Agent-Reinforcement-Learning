# 6.3 Setting

이전에서 고려하던 것과 같은 Cooperative MARL in partially observable상황에서의 실험을 진행합니다. 모든 agent는 reward를 sharing하고 이 목표를 maximizing하는데 목표를 둡니다. 그리고 Markov state $$s_t$$를 직접 관측하지 못하고 이에 correlated된 $$o^a_t$$를 관측합니다. 딱 이런 상황에서 agent 끼리의 협력하기 위한 communication이 장려된다는 점을 알 수 있습니다. 여기서 통신을 위해 사용한 방법은 chapter와는 다르게 environment가 cheap-talk channel을 포함합니다. 각 time step마다 agent는 environment에 직접 영향을 주는 environment action $$u \in U$$를 선택하고, 다른 agent들에게 관측되지만 실제 environment에 영향을 주지않고 reward도 받지않는 communication action $$ m \in M$$을 선택합니다. communication protocol이 이전에 주어지지 않음에 따라 agent는 문제를 해결하기 위해 communication protocol를 발전시키고 서로 동의해야합니다. 

 이러한 communication protocol 학습에 걸림돌이 되는 점은 자체가 action-observation histories로 부터 mapping되기 때문에, protocol의 dimension이 엄청나게 큽니다. 그렇기에 자동적으로 효과적인 protocol을 찾는 것이 어려워집니다. 또한, communication protocol space에서의 exploration도 간단한 문제가 아니게 됩니다. 예를 들면, 한 agent가 유용한 message를 다른 agent에게 전달하면, 이느 이 자체로 좋은 reward를 받아야 하지만, 보통 reward는 sparse하고, 잘 전달된 것을 넘어서 좋은 협력을 해냈을 때 보통 좋은 reward를 받기 때문입니다. 



