# 6.3 Setting

이전에서 고려하던 것과 같은 Cooperative MARL in partially observable상황에서의 실험을 진행합니다. 모든 agent는 reward를 sharing하고 이 목표를 maximizing하는데 목표를 둡니다. 그리고 Markov state $$s_t$$를 직접 관측하지 못하고 이에 correlated된 $$o^a_t$$를 관측합니다. 딱 이런 상황에서 agent 끼리의 협력하기 위한 communication이 장려된다는 점을 알 수 있습니다. 여기서 통신을 위해 사용한 방법은 environment에서 cheap-talk channel을 사용하는 방법입니다. 각 time step마다 agent는 environment에 직접 영향을 주는 environment action $$u \in U$$를 선택하고, 다른 agent들에게 관측되지만 실제 environment에 영향을 주지않고 reward도 받지않는 communication action $$ m \in M$$을 선택합니다. communication protocol이 사전에 정의되지 않음에 따라 agent는 문제를 해결하기 위해 communication protocol를 발전시키고 서로 동의해야합니다. 

 이러한 communication protocol 학습에 걸림돌이 되는 점은 자체가 action-observation histories로부터 mapping되기 때문에, protocol의 dimension이 엄청나게 큽니다. 또한 한 agent가 유용한 message를 다른 agent에게 전달하면, 이느 이 자체로 좋은 reward를 받아야 하지만, 보통 reward는 sparse하고, 잘 전달된 것을 넘어서 좋은 협력을 해냈을 때 보통 좋은 reward를 받기 때문에 학습 자체도 굉장히 어렵고, communication protocol space에서의 exploration도 고려해야하기 때문에, communication protocol learning은 굉장히 어려운 일입니다.



