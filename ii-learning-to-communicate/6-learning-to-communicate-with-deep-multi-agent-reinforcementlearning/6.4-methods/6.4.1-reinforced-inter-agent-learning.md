# 6.4.1 Reinforced Inter-Agent Learning

가장 직접적인 방법으로, reinforced inter-agent learning\(RIAL\)을 제시합니다. 이는 DRQN과 IQL을 함께 적용했습니다. 그렇다면 이 agent의 Q-network의 형태는 다음과 같습니다. 

                                                                      $$Q^a(o^a_t,m^{a'}_{t-1},h^a_{t-1},u^a)$$

Q function은 다음과 같이 observation $$ o^a_t$$, message $$ m^{a'}_{t-1}$$, 이전의 hidden state $$h^a_{t-1}$$, 그리고 agent의 action $$u^a$$로 이루어집니다. agent의 action은 environment action과 communication action 두 가지 action으로 이루어져있기 때문에, 이 action space는 combinatorial하게 $$|U||M|$$이 될 수 있으나, 아예 branch를 분리하여 $$ |U|+|M|$$하게 구성하였습니다.\(action space 감소 목적\) 그리하여 action selector가 $$u^a_t$$와 $$m^a_t$$를 각각 $$ \epsilon - \mathrm{greedy}$$하게 뽑게 됩니다. 

$$ Q_u$$와 $$Q_m$$은 다음과 같이 두가지 변형이 된 DQN에 따라 학습이 되는데, 이는 성능을 내기 위해 꼭 필요한 테크닉인 것을 실험적으로 보았습니다.

* 첫째로, experience replay를 비활성화했습니다. 이는 다른 agent들에 의한 non-stationary가 학습에 치명적임을 발견하였기 때문입니다.
* 둘째로, 이전 agent의 action u와 m을 각 agent의 input으로 넣어줍니다.

### Parameter Sharing

RIAL은 centralized learning의 장점을 agent간의 parameter sharing으로 가져가는 방법으로 확장될 수 있습니다. 그리고 agent마다 모두 자신만의 index a를 input으로 가지는 network를 구성했습니다. 이렇게 했을 때, common policy를 만들면서도 agent마다의 specialization을 유지할 수 있었습니다. parameter sharing을 통해 agent의 trainable parameter 개수를 줄임으로써 학습 속도를 빠르게 만들었는데 이를 통해 Q function을 다시 나타낸다면 다음과 같습니다. 

                                              $$ Q_u(o^a_t,m^{a'}_{t-1},h^a_{t-1},u^a_{t-1},m^a_{t-1},a,u^a_t)\  \ \ \ \mathrm{and} \ \ \ Q_m(\cdot)$$

decentralized execution때, 각 agent는 복제된 network를 사용하지만 각자의 agent가 관측한 observation을통해 hidden state를 갱신하고, action을 고르고 communication하게 됩니다.

