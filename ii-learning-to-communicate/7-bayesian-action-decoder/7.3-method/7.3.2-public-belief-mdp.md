# 7.3.2 Public Belief MDP

public belief를 update하는 방법에 대해 알아보도록 하겠습니다. public belief는 다음처럼 나타낼 수 있습니다.

                                                                      $$P(f^a_t|u^a_t,\mathcal{B}_t,f^\mathrm{pub}_t,\hat{\pi})$$

$$\hat{\pi}$$와 $$u^a_t$$는 public information이므로, 한 agent가 관측한 것 $$u^a_t$$을 통해 이후의 가능한 private state features $$f^\mathrm{pri}_t$$의 확률이 바로 public belief입니다. 이는 다시 다음과 같이 표현할 수 있습니다.

                                                $$ P(f^a_t|u^a_t,\mathcal{B}_t,f^\mathrm{pub}_t,\hat{\pi}) = \frac{P(u^a_t|f^a_t,\hat{\pi})P(f^a_t|\mathcal{B}_t,f^\mathrm{pub}_t)}{P(u^a_t|\mathcal{B}_t,f^\mathrm{pub}_t,\hat{\pi})}$$        

 이는, $$ \mathcal{B_t}$$와 $$f^\mathrm{pub}_t$$를 알고 $$\hat{\pi}$$가 뽑힌 상황에서$$u^a_t$$가 뽑혔을 때, 이를 $$ \mathcal{B_t}$$와 $$f^\mathrm{pub}_t$$를 안상태에서 관측할 확률과 $$f^a_t$$와 $$\hat{\pi}$$를 안상태에서 $$ u^a_t$$를 뽑을 확률과 같습니다.                     

그리고 이는 당연히 $$\propto \bm{1}(\hat{\pi}(f^a_t),u^a_t)P(f^a_t|\mathcal{B}_t,f^\mathrm{pub}_t)$$\(indicator\)입니다. 

이를 통해 우리는 새로운 MDP인 PuB-MDP를 생각할 수 있습니다. 아래 그림의 \(b\)를 살펴보겠습니다.

![](../../../.gitbook/assets/marl_21.png)

 PuB-MDP의 state 에 대해 $$s_{\mathrm{BAD}} \in S_{\mathrm{BAD}}$$는 public observation과 public belief로 이루어져있습니다. deterministic partial policies는 private observation을 통해 action 으로 mapping합니다. 이를 transition probability로 나타내면 다음과 같습니다.

                                                                          $$P(s'_{\mathrm{BAD}}|s_\mathrm{BAD},\hat{\pi})$$

다음 state는 새로운 public belief를 가지고 있으니, 이를 public belief update식을 통해 구할 수 있습니다. 이때,  일반 MDP에서는 action에 의해 transition probability가 정의되지만 여기서는 $$ \hat{\pi}$$에 의해\(private observation에 따른 실행되지 않은 action모두가 transition probability에 관여합니다.\) 정의되는 것을 볼 수 있습니다. 

 reward function은 private state feature에 대한 marginality를 위해 다음과 같이 구성합니다.

                                                     $$ r_{\mathrm{BAD}}(s_{\mathrm{BAD}},\hat{\pi}) = \sum_{f^{pri}}{\mathcal{B}(f^\mathrm{pri})r(s,\hat{\pi}(f^\mathrm{pri}))}$$

 이는 private observation에 대한$$\mathcal{B}_t$$를 reward에 곱해 계산합니다.

