# 7.3.1 Public belief

Single-agent partially observable상황에서는 agent가 보이지 않는 환경에 대한 자신만의 belief 갖는 것이 꽤 유용했습니다. 하지만 Multi-agent 상황에서는 다른 agent들도 볼수 없는 state가 있기 때문에 환경에 대한 belief을 혼자 가지고 있는 것이 더 이상 충분하지 않습니다. Interactive POMDPs\(I-POMDPs\)에선 agent는 서로에 대한 belief 그러한 belief에 대해 modeling해내는데, 이는 계산적으로 불가능한 부분들이 있습니다.

하지만 운좋게도 여기에서의 연구는 위에서 말한 common knowledge가 public belief를 계산가능하게 해주는데, 이는 I-POMDPs의 재귀를 불필요하게 만듭니다. 이 때,  public belief $$\mathcal{B}_t$$는 모든 이전의 알려진 public features로부터의 새로운 private state features입니다. 이는 다음과 같이 나타낼 수 있습니다.

                                                        $$ \mathcal{B}_t = P(f^{pri}_t|f^{pub}_{\leq t})$$

$$ \mathcal{B}_t$$는 public하게 알려진 정보에 대해서, 각 agent로부터 모두에게 알려진 algorithm을 사용해 독립적으로 이 public belief를 만들게 됩니다. public belief가 재귀적인 추론을 안하게 되면, 어떻게 agent의 action은 어떻게 결정되는지 의문이 들 것입니다. 위의 설명대로한다면 agent는 그들의 private observation을 이용하지 않고, public belief만 이용해 추론을 할 것이 당연해지는데, 이렇게 public observation과 public belief을 통해 학습되어도 이는 optimal policy를 찾을 수 있다고 말합니다. 그 이유는 $$ \pi_{\mathrm{BAD}}$$가 partial policy\(자신의 관측을 사용할 수 있는 policy\)를 골라 이 policy가 action을 고르기 때문입니다. 이는 다음과 같이 나타낼 수 있습니다. 

                                                             $$ \hat{\pi} : \{ f^a \rightarrow \mathcal{U} \}$$

이렇게 partial policy는 deterministic하게 선택됨으로써, policy는 high entropy entropy를 가지게 학습이 되, communication은 low entropy를 가져야하는 특성들을 모두 만족시키게 됩니다.

직관적으로 public agent는 제오직 public observation과 belief를 관찰하는 3자처럼 볼 수 있습니다. $$ \pi_{\mathrm{BAD}}$$가 private state를 보지 못하므로, 이는 각 agent에게 어떤 private observation을 받았을 때 어떻게 하라는지 알려줄 수있습니다. 즉 각 time step에서 public agent는 $$\mathcal{B}_t$$와 $$ f^{pub}_t$$에 기반해 partial policy $$ \hat{\pi}$$를 선택합니다. 그런 다음, partial policy$$ \hat{\pi}$$는 자신의 private state를 이용해 action을 선택합니다.

                                                                $$ \hat{\pi}(f^a) = u^a_t$$ 

그 다음 public agent는 observed action $$u^a_t$$를 이용해 새 belief $$\mathcal{B}_{t+1}$$를 만듭니다.

