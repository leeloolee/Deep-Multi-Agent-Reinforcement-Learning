# 7.3.1 Public belief

Single-agent partially observable상황에서는 agent가 보이지 않는 환경에 대한 자신만의 belief 갖는 것이 꽤 유용했습니다. 하지만 Multi-agent 상황에서는 다른 agent들의 관측 상황\(혹은 policy\)등에 따라 MDP는 변하게 되고, 그런 belief는 쉽게 깨질 수 있으므로, 환경에 대한 belief을 혼자 가지고 있는 것이 더 이상 충분하지 않습니다. Interactive POMDPs\(I-POMDPs\)에선 agent는 서로에 대한 belief 그러한 belief에 대해 modeling해내는데, 이는 계산적으로 불가능한 부분들이 있습니다.

 public belief $$\mathcal{B}_t$$는 모든 이전의 알려진 public features로부터의 한 agent의 새로운 private state features입니다. 이는 다음과 같이 나타낼 수 있습니다.

                                                                            $$ \mathcal{B}_t = P(f^\mathrm{pri}_t|f^\mathrm{pub}_{\leq t})$$

$$ \mathcal{B}_t$$는 모두에게 알려진 정보에 대해서, 즉 각 agent로부터 모두에게 알려진 algorithm을 사용해 독립적으로 같은 public belief를 만들 수 있는 정보에 의해 만들어집니다. 그렇다면 agent는 그들의 private observation을 이용하지 않고, public belief만 이용해 추론하는 상황을 어떻게 해결할 수 있을지에 대한 고민을 해야합니다. 이는 Nayyar가 제안한대로 public observation과 public belief을 통해 학습하면 이는 optimal policy를 찾을 수 있습니다. 그 이유는$$ \pi_{\mathrm{BAD}}$$가 partial policy\(자신의 관측을 사용할 수 있는 policy\)를 골라 이 policy가 action을 고르기 때문입니다. 이는 다음과 같이 나타낼 수 있습니다. 

                                                                               $$ \hat{\pi} : \{ f^a \rightarrow \mathcal{U} \}$$

이렇게 partial policy는 deterministic하게 선택됨으로써, policy gradient로 communication protocol은 high entropy entropy를 가지게 학습이 되고, communication은 low entropy를 가지게 학습이 되는 특성을 가질 수 있게 됩니다.

직관적으로 public agent는 오직 public observation과 belief를 관찰하는 3자처럼 볼 수 있습니다. $$ \pi_{\mathrm{BAD}}$$가 private state를 보진 못하지만 각 agent에게 어떤 private observation을 받았을 때 어떻게 하라는지 알려줄 수 있습니다. 즉 각 time step에서 public agent는 $$\mathcal{B}_t$$와 $$ f^\mathrm{pub}_t$$에 기반해 partial policy $$ \hat{\pi}$$를 선택합니다. 그런 다음, partial policy$$ \hat{\pi}$$는 자신의 private state를 이용해 action을 선택합니다.

                                                                $$ \hat{\pi}(f^a) = u^a_t$$ 

그 다음 public agent는 observed action $$u^a_t$$를 이용해 새 belief $$\mathcal{B}_{t+1}$$를 만듭니다.

