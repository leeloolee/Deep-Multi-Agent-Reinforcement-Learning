# 7.4.5 Architecture Details for Baselines and Method

여기서는 학습을 위해 Importance-Weighted Actor-Learner Architecture를 사용하였는데 이 framework에서 actor는 계속 trajectories를 만들고, learner는 gradient update만 진행하는 구조입니다. trajectories를 만드느라 사용된 policy는 gradient update할 때 여러번 사용될 수 있는데, 이에 대한 분포를 맞춰주기 위해 V-trace가 사용되었습니다.

V0-LSTM과 V1-LSTM agent는 MLP를 거쳐 2-layer의 LSTM을 지나는데, policy $$\pi$$는 이에 LSTM의 output에 softmax를 통해 얻습니다. baseline network는 MLP로만 구성되었고 이는 하나의 값을 output으로 가집니다. 이는 gradient update만을 위해 사용되었기 때문에, Chapter 3에 나오는 centralized critic형태를 따라 만들었고, 이는 모든 agent의 private observation을 받습니다.

BAD agent는 모든 observation을 input으로 가지는 MLP로 구성되어 있으며, baseline을 계산하기 위해, 같은 MLP를 가졌지만, agent의 자신의 패까지볼 수 있도록 만들었습니다.

 모든 agent에 대해 illegal action은 logit을 큰 negative value를 줌으로써 masking을 하여 sampling되지않도록하였고, no-action외엔 다른 action은 가만히 있을 수 없습니다.

