# 9.1 Introduction

 이전 chapter에서 다른 agent들의 학습행동을 자신의 optimization term에 넣는 Learning with Opponent-Learning Awareness\(LOLA\)에 대해 배워보았습니다. 이렇게 하기 위해서 agent는 다른 agent의 learning step을 미분할 수 있어야 했습니다. 이는 optimization과정에서 높은 차수의 gradient를 발생시키는 것을 보았습니다. 하지만 이 때, objective는 미분 불가하고\(Go를 예로 들었을 때, 이겼을 때 1, 졌을 때 -1 이라면, 이 reward는 당연히 불연속적이고 미분 불가\) 이를 sampling을 통해 monte-carlo-estimation을 하게 됩니다. first order gradient는 policy gradient에서 많이 쓰이는 score function trick\($$\nabla \log(\pi)$$\)를 사용해 추정 가능합니다. 하지만 gradient estimator가 직접적으로 정의되면 미분값이 gradient estimator가 되는 objective를 더 편하게 정의 가능하고, pytorch나 tensorflow 같은 auto-diff deep learning library에게 이를 맡기면 됩니다.

 이 접근은 stochastic computation graph\(SCG\)로부터 surrogate objective를 생성하는 surrogate loss\(SL\)를 사용하는 method로, \(이 용어들이 처음들어본다면 9.2 Background를 먼저 읽고 오시는것이 도움이 될 수 있습니다.\) 미분이 될 때, SL가 원래 objective의 first of gradient의 추정값을 내놓게 됩니다. 

 그러나 높은 차수의 gradient는 더 어려울 수 있습니다. LOLA에서 원하는 것을 넘어서 이런 estimator는 다른 여러 optimization technic에 유용할 수 있으며, 수렴을 가속할 수 있습니다.이는 meta-learning에서도 유용하게 사용됩니다.

 하지만 위에서 언급한 1차 gradient는 높은 차수의 미분값은 auto-diff library에 잘 맞지 않습니다. 이는 sampling distribution에 의존하기 때문에, 높은 차수의 gradient estimator도 똑같은 score function trick을 사용해야합니다. 그냥 1차 estimator를 미분하는 것은 이미 Finn의 연구결과에서 잘못된 term을 가져온다는 것을 보았습니다.

 높은 차수의 socre function gradient estimator를 구하는데 만족되지 않는 두가지 점이 존재합니다.

* 첫째로, estimator를 sampling같은 방법이 아닌 analytical하게 유도하거나 구현해야합니다. 이는 번거롭고 오류가 나기 쉽고 auto-diff에 적용되기 어렵습니다.
* 둘째로, 새로운 objective를 위한 SL의 반복적인 적용을 적용하는 방법인데, 이는 점점 복잡한 그래프를 수반하게 됩니다.

 미분한 후 1차 gradient를  맞추기 위해선 SL은 cost를 고정된 sample처럼 처리해야합니다. 여기에선 이것이 높은 차수의 gradient estimator로 갔을 때, 얼마나 비정확한 term을 만드는지 보입니다. 이는 높은 차수의 gradient가 적용되야하는 method의 적용 범위를 제한하게 됩니다.

 여기에선 위의 문제점들을 해결하기 위해 Infinitely Differentiable Monte-Carlo Estimator\(DiCE\)를 제시합니다. DiCE는 원래의 objective를 추정하기 위한 하나의 objective를 만드는데, 이는 어느 차수에서도 정확한 미분값을 얻을 수 있다는 특징이 있습니다. SL의 접근방법과는 다르게 DiCE는 높은 차수의 gradient계산을 auto-diff에 의존합니다.

 Dice는 Operator MAGICBOX\(여기서는 $$\square$$로 표현하겠습니다.\)를 정의합니다. SCG에서 original loss에 영향을 미치는 stochastic node $$\mathcal{W}_c$$를 모두 포함합니다. 만약 미분이 일어난다면, 이 MagicBox는 sampling distribution에 대한 정확한 gradient를 내놓습니다.

                                              $$ \nabla_{\theta}\square(\mathcal{W}_c) = \square(\mathcal{W}_c)\nabla_{\theta}\sum_{w\in \mathcal{W}_c}{\log(p(w;\theta))}$$

이 MagicBox를 $$\mathcal{W}$$에 대해 측정하자면 1을 return합니다.  

                                                                                $$ \square(\mathcal{W}) \rightarrow 1$$

 위의 결과가 나오기 위해서 MagicBox operator는 auto-diff library에서 다음과 같이 쉽게 구현가능합니다.

                                                                $$ \square(\mathcal{W}) = \exp(\tau-\bot(\tau))$$

                                                                  $$ \tau = \sum_{w \in \mathcal{W}}{\log(p(w;\theta)}$$

 $$ \bot$$는 $$ \nabla_x\bot(x)=0$$이 되는 operator입니다. 이후에 어떻게 baseline을 통해 variance를 줄이는지에 대해 보입니다.

이번 chapter내에서 DiCE의 증명과 수치의 평가를 통해 정확성에 대해 보입니다. 또한, LOLA에 DiCE를 적용한 모습도 보입니다.



