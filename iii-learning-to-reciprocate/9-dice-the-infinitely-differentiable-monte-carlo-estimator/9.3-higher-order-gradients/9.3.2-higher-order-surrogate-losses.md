# 9.3.2 Higher Order Surrogate Losses

Schulman은 1차 미분에만 집중했고 높은 차수의 미분에 대해 할수있다만 제시했었습니다. 또한 cost와 parameter간의 의존성을 끊어서 높은 차수의 미분이 좀더 간단해졌습니다.

 다음 보겠습니다. single parameter $$\theta$$에 대해 sampling distribution은 $$p(x;\theta)$$로 정의되고 objective는 $$f(x,\theta)$$입니다.

                                                         $$ SL(\mathcal{L}) = \log p(x;\theta)\hat{f}(x) + f(x;\theta) $$

                                                                 $$( \nabla_\theta\mathcal{L})_\mathrm{SL}= \mathbb{E}_x[\nabla_\theta\mathrm{SL}(\mathcal{L})]$$

                                                                                     $$ = \mathbb{E}_x[\hat{f}(x)\nabla_{\theta}\log p(x;\theta) + \nabla_\theta f(x;\theta)]$$

                                                                                     $$ = \mathbb{E}_x[g_\mathrm{SL}(x;\theta)]$$

이 때, 9.3.1에서 본 아래의 수식과 첫번째 term의 의존성이 다른 것을 볼 수 있습니다.

                                                         $$\mathbb{E}[f(x;\theta)\nabla_{\theta}\log{p(x;\theta)}+\nabla_{\theta}f(x;\theta))] \cdots(9.3.1)$$

 이는 같이 같은 1차 gradient를 근사하더라도 함수간의 의존성이 부족한 것은 정확한 2차 미분과의 괴리를 만들어 낼 수 있습니다.

                                                    $$ SL(g_{\mathrm{SL}}(x;\theta)) = \log p(x;\theta)\hat{g}_{\mathrm{SL}}(x) + g_{\mathrm{SL}}(x;\theta)$$

                                                    $$( \nabla^2_\theta\mathcal{L})_\mathrm{SL} = \mathbb{E}_x[\nabla_\theta\mathrm{SL}(g_{\mathrm{SL}})]$$

                                                                        $$ = \mathbb{E}_x [\hat{g}_{\mathrm{SL}}(x)\nabla_\theta\log p(x;\theta) + \nabla_{\theta}g(x;\theta)]$$

 $$g_{\mathrm{SL}}(x;\theta)$$는 $$g(x;\theta)$$와 $$\theta$$에 의존성에 관한 조그만 차이가 있을 뿐입니다. 그래서 이 때 지금의 값은 같습니다. 하지만 미분을 더 진행할 때, 이는 큰 차이가 드러납니다.

             $$ \nabla_{\theta}g(x;\theta) = \nabla_{\theta}f(x;\theta)\nabla_{\theta}\log(p(x;\theta)) + f(x;\theta)\nabla^2_\theta \log(p(x;\theta)) + \nabla^2_\theta f(x;\theta)$$

                                     $$ \nabla_\theta g_{\mathrm{SL}}(x;\theta) = \hat{f}(x)\nabla^2_\theta\log(p(x;\theta))+\nabla^2_\theta f(x;\theta)$$

아래 $$g_\mathrm{SL}$$에 관한 식에선 $$ f(x;\theta)\nabla^2_\theta \log(p(x;\theta))$$term을 잃어버리게 됩니다. 하지만 앞에서도 말했듯이 $$g$$또한 수렴하지 않는다는 것이 Finn의 연구에서 밝혀졌습니다.

