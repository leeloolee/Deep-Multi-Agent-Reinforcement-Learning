# 8.6 Conclusion & Future Work

 이번 section에서는 Opponent-Learning Awareness\(LOLA\)에 대해 알아 보았습니다. 이는 MARL상황에서 다른 agent의 학습을 고려해 자신의 학습을 하는 방법입니다. 여기서는 value function에 대해 접근할 수 있을 때 IPD에서 NL은 defact 전략에 수렴하는 경향을 보였지만 LOLA는 tit-for-tat 전략이 우위를 점하는 것을 보았습니다. 또한, LOLA가 IMP에서도 내쉬 균형을 이루는 것을 보았습니다. 다른 multi-agent learning algorithm과도 IPD와 IMP에서 실험적으로 좋은 결과를 보이는 것을 확인했습니다. 

 value function에 직접 접근하지 못할 때에 대해 gradient-based version LOLA를 소개하고, Coin Game에서 이의 학습을 보입니다. 이 때, recurrent layer가 필요함을 보았고, LOLA는 coordination하는 경향을 띄는 것을 보았습니다. 이때, opponent의 parameter 정보가 없어도 이를 어느정도 해결할 방법에 대해 설명했고, LOLA의 high-order approximation에 대해 보았습니다. 이는 IPD에서 agent 모두 LOLA를 사용하는 것이 좋은 성능을 보였고 high order approximation로 얻는 추가적인 소득은 없었습니다.

저자는 이후 future work로 적대적인 agent가 gradient-based method가 아닌, global search method를 통해 LOLA를 이용하려 들 때에 어떻게 LOLA의 취약점을 해결할 수 있을지에 대해 해결하는 것을 연구하겠다고 합니다. LOLA가 naive learner를 이용하는 방법인 만큼 LOLA learner를 이용할 수단이 있을 것이 타당성 있기 때문입니다.

