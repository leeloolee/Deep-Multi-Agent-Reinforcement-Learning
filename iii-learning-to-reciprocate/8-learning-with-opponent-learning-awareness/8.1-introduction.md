# 8.1 Introduction

이전 장에서는 오로지 fully cooperative multi-agent RL에 대해서 배웠습니다. 하지만 당연하게도 Multi-Agent에서의 문제는 항상 cooperative하지 않는 것을 알 수 있습니다. Hierarchical Reinforcement Learning이나, Generative Adversarial Network와 Decentralized Optimization같은 분야들도 Multi-agent Problem으로 볼 수 있는데, 이러한 모든 설정에서 특히 Trainable Object간의 다른 objective를 배울 때, 문제가 non-stationary해지고, 불안정해지거나, 바라지 않았던 결과를 얻게 됩니다. 

다양하고 복잡한 상황에서도 협력을 유지하는 능력은 인간 사회의 성공에 큰 기여를 했습니다. 전쟁 속에서도 이러한 능력은 관찰되곤 했습니다. 

미래에 AI agent가 인간사회에서 부분적으로 협력해야하는 상황에서 적극적으로 활용이 될 것으로 기대되는데, 이 때 agent의 학습 실패는 큰 재앙이 될 것입니다.

 각자의 reward를 최대화 하려는 agent들 끼리의 상호 협력은 어떻게 일어나는 것인지에 대해 대해서도 많은 궁금증이 있었습니다. 게임 이론에서 협동적이고 경쟁적인 요소를 포함하는 게임에서 학습 결과를 연구한 오랜 역사를 가지고 있습니다. 특히 협력과 변절에 대해 iterated prisoner's dilemma 문제에서 많이 다루어졌습니다. 이 게임에서의 이기주의는 모든 agents의 전체적인 reward의 감소로 이루어집니다. 하지만, 협력은 전체의 reward를 좋게 만듭니다.

 흥미롭게도, 이러한 간단한 죄수의 딜레마 문제에서도, RL agent가 학습하면, 변절을 하도록 하는데, 이는 현재 DMARL의 SOTA 또한 이러한 간단한 협력문제를 해결하지 못할 수 있다는 것을 의미합니다. 이는 다른 agent들을 단지 환경의 일부분이라고 생각하는 것이 문제가 됩니다. 

 다른 agents의 행동에 대해 학습하는 행동에 대한 의미를 추론하는 단계로써, 여기서는 Learning with Opponent-Learning Awareness\(LOLA\)를 제안합니다. LOLA는 다른 agent의 parameter update가 다른 agent의 학습에 어떤 영향을 끼치는지 설명할 수 있는 추가적인 term을 가진 학습 룰을 포함합니다. 반복된 설명을 막기위해 zero-sum 상황에 한정되지 않더라도 상대방들을 모두 opponents라고 표현하겠습니다. 여기서는 iterated prisoner's dilemma\(IPD\)상황에서 모든 agent에게 적용되는 추가적인 term을 이용해 상호작용하고 협력할 수 있도록 함을 보였습니다. IPD에서 실험적으로 보았을 때, 각 agent는 higher order gradient terms와 사용하는 LOLA가 추가적인 보상이 없이도, naive learning에서 LOLA로 바뀌는 것을 장려되는 것을 보았습니다. 이는 local space에서 gradient-based learning rule은 모든 

마지막으로, grid-world task에 opponent modelling이 있고 없고에 따른 LOLA를 적용한 실험을 진행하였습니다. 이 task는 일시적으로\(?\) action을 확장하고 high-dimensional recurrent policies를 필요로 했습니다.

