# 8.4.3 Training Details

Gradient-based NL과 LOLA을 학습할 때엔, actor-critic method를 적용하였습니다. 

1. 학습 중간에는 learning rate $$\delta$$를 actors학습할 땐 0.005, critic엔 1을 사용했다고 하는데, td error로 update하므로 당연히 1을 주는게 맞다고 생각했습니다.
2. discount rate $$\gamma $$는 IPD와 Coin Game에서  0.96, matching pennies에서 0.9를 사용했습니다.
   1. 이 때, IPD와 Coin Game에서 사용한 높은 $$\gamma $$값은 긴 time-step에 대한 reward signal을 받을 수 있게 하기 위함입니다.
   2. IMP에선 낮은 $$\gamma $$값이 학습을 좀 더 안정성있게 만들었습니다. 

 리그전을 위해 baseline algorithm과 parameter들은 자세하게는 본문을 확인하시면 편합니다. 리그전은 모든 agent가 1000 에피소드씩 200step을 진행하였습니다.

