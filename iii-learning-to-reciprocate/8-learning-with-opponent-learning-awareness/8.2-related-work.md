# 8.2 Related Work

general-sum game에 대한 연구는 게임이론과 진화 연구에서 많이 이루어졌습니다. 많은 논문에서 IPD를 해결했는데, 특히 Axelrod의 연구에 주목해보겠습니다. 이 연구는 tit-for-tat의 대중화를 이끌었는데, 이는 효과적이면서도 간단한 전략으로 agent가 처음엔 협력적으로 행동하고, 이후에는 opponent의 최근 행동을 따라하는 전략입니다. 

 많은 MARL 연구는 agent 스스로 학습해 수렴하고, 순차적인 general sum game에서 합리성을 얻는 것을 목표를 하고 있습니다. 그런 연구에는 WoLF algorithm, joint-action-learner\(JAL\)과 AWESOME이 있습니다. LOLA와는 다르게 이런 algorithms은 주어진 제약조건들에 대해 수렴하는 행동을 잘 이해하고 있습니다. 그러나 이런 algorithm은 전체적으로 더 높은 reward에 수렴하기 위해서 opponent의 학습하는 행동에 대해 알아내는 능력이 없습니다. WoLF는 agent가 이기고 지는 것에 learning rate를 다르게 하여 학습을 진행합니다. AWESOME은 iterated game의 일부분인 한번에 끝나는 game에 대해 배우기 위하는 것에 목표를 둡니다.  
general-sum상황에서 JAL의  dynamics를 분석하기 위한 연구들로 Uther의 zero-sum 상황에서의 연구와 Claus의 cooperative 상황에서의 연구가 있습니다. Sandholm은 IPD에서 다양한 exploration전략을 가지고 function approximator를 가진 IQL에 대해 연구하였습니다. Wunder와 Zinkevich는 iterated game에서 dynamics의 수렴과 학습의 평형상태에 대해 연구했으나 LOLA와 다르게 학습하는 전략에 대해 제시하지 않았습니다.

Littman은 각 opponent를 fully cooperative 혹은 fully adversarial하게 가정하고 해결하였는데, LOLA는 이를 단지 general-sum game임만을 고려해서 해결할 수 있습니다.

 Chakraborty는 policy를 여러개 두고 최적의 반응에 대해 배우는데 LOLA는 하나의 policy로 해결하였습니다.

Brafman의 연구에선 efficient learning equilibrium\(ELE\)라는 개념을 소개하는데, 이 algorithm에서는 모든 내쉬균형이 계산되어야합니다. LOLA에서는 그런 가정이 필요하지 않습니다.

 DMARL에선 주로 fully cooperative나 zero-sum 환경과\(이들의 reward는 측정하기 쉬운편\) communication이 필요한 상황에 대해 많은 연구가 이루어졌습니다. 하지만 Leibo의 연구는 partially observable, general sum 상황에서 naive learning에 대해 연구하였고, Lowe도 general sum 상황에 대한 centralized actor-critic architecture를 제안하였습니다. 이 두 연구에서는 다른 agent의 학습 행동에 대한 추론을 할 방법을 제시하지 못했습니다. Lanctot은 NFSP같은 game-theoretic best-response-style algorithm의 아이디어를 일반화하였습니다. 이는 주어진 opponent의 policies에 대한 set이 필요하지만 LOLA는 opponent의 학습에 대해 어떤 가정도 필요하지 않습니다. 

 Lerer의 연구가 가장 LOLA과 비슷한데, 이는 tit-for-tat을 DMARL를 통해 일반화하였습니다. 이 저자는 agent 모두 fully cooperative와 defecting 하는 policy를 배우며, 이를 바꿔가며 학습해 tit-for-tat 전략을 수립하도록 합니다. 이 연구와 비슷하게 Munoz도 repeated stochastic game에서 competitive와 cooperative를 바꿔가며 egalitarian equilibrium을 찾는 내쉬 균형 알고리즘을 제안했습니다. 비슷한 아이디어로 M-Qubed에서는 최적의 반응, 신중한 반응, 그리고 optimistic learning biases의 균형을 맞춥니다.

이런 algorithm들은 상호 작용이나 협력이 algorithm내에서 발생하지 않고, heuristic하게 발생되는데, 이는 이런 algorithm들의 일반화에 큰 제약을 줍니다.

여기서의 opponent modeling와 연관된 연구는 fictitious play와 action-sequence prediction이 있습니다. Meanling은 memory를 이용해 opponent의 future action을 예측해 policy를 찾는 방법을 제시하였습니다. 게다가 Hernandez-Leal은 상대의 직접적으로 상대의 distribution에 대해 modeling을 합니다. 이런 방법들이 opponent에 대한 전략을 modeling하고 최적의 반응에 대한 policy를 찾는데 집중한 반면, opponent의 학습에 대한 dynamic을 배우는데 까지는 해결하지 못했습니다. 

 반면에 Zhang의 연구에서는 one-step learning dynamics에 대한 policy prediction을 사용하는데, 이는 opponent의 policy update가 주어진다고 가정하고, 그에 맞는 최적에 선택을 배웁니다. LOLA는 이와 다르게 직접적으로 opponent의 policy의 학습을 드러내고, 자신의 reward를 최적화할 때 사용합니다. LOLA에서 유일하게 사용된 opponent의 learning step을 미분하는 것은 이러한 상호 협력 혹은 tit-for-tat의 등장에 중요한 역할을 합니다. 이는 DMARL에서 최초로 시도하는 방법입니다.

 LOLA는 상대방의 policy update를 미분하여 사용합니다. 이는 Metz가 제안한 아이디어와 비슷하긴한데, 이는 GAN을 학습시킨 방법으로, 전체적인 효과는 비슷합니다. opponent의 학습 프로세스를 미분하는 것은 전체적인 zero-sum game의 학습을 안정화합니다.







