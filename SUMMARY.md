# Table of contents

* [Deep Multi-Agent Reinforcement Learning](README.md)

## Abstract & Contents

* [Abstract](abstract-and-contents/deep-multi-agent-reinforcement-learning.md)

## 1. Introduction

* [1. INTRODUCTION](1.-introduction/1.-introduction/README.md)
  * [1.1 The Industrial Revolution, Cognition, and Computers](1.-introduction/1.-introduction/1.1-the-industrial-revolution-cognition-and-computers.md)
  * [1.2 Deep Multi-Agent Reinforcement-Learning](1.-introduction/1.-introduction/1.2-deep-multi-agent-reinforcement-learning.md)
  * [1.3  Overall Structure](1.-introduction/1.-introduction/1.3-overall-structure.md)

## 2. Background

* [2. BACKGROUND](2.-background/2.-background/README.md)
  * [2.1 Reinforcement Learning](2.-background/2.-background/2.1-reinforcement-learning.md)
  * [2.2 Multi-Agent Settings](2.-background/2.-background/2.2-multi-agent-settings.md)
  * [2.3 Centralised vs Decentralised Control](2.-background/2.-background/2.3-centralised-vs-decentralised-control.md)
  * [2.4  Cooperative, Zero-sum, and General-Sum](2.-background/2.-background/2.4-cooperative-zero-sum-and-general-sum.md)
  * [2.5 Partial Observability](2.-background/2.-background/2.5-partial-observability.md)
  * [2.6 Centralised Training, Decentralised Execution](2.-background/2.-background/2.6-centralised-training-decentralised-execution.md)
  * [2.7 Value Functions](2.-background/2.-background/2.7-value-functions.md)
  * [2.8 Nash Equilibria](2.-background/2.-background/2.8-nash-equilibria.md)
  * [2.9 Deep Learning for MARL](2.-background/2.-background/2.9-deep-learning-for-marl.md)
  * [2.10 Q-Learning and DQN](2.-background/2.-background/2.10-q-learning-and-dqn.md)
  * [2.11 Reinforce and Actor-Critic](2.-background/2.-background/2.11-reinforce-and-actor-critic.md)

## I Learning to Collaborate

* [3. Counterfactual Multi-Agent Policy Gradients](i-learning-to-collaborate/3.-counterfactual-multi-agent-policy-gradients/README.md)
  * [3.1 Introduction](i-learning-to-collaborate/3.-counterfactual-multi-agent-policy-gradients/3.1-introduction.md)
  * [3.2 Related Work](i-learning-to-collaborate/3.-counterfactual-multi-agent-policy-gradients/3.2-related-work.md)
  * [3.3 Multi-Agent StarCraft Micromanagement](i-learning-to-collaborate/3.-counterfactual-multi-agent-policy-gradients/3.3-multi-agent-starcraft-micromanagement.md)
  * [3.4 Methods](i-learning-to-collaborate/3.-counterfactual-multi-agent-policy-gradients/3.4-methods/README.md)
    * [3.4.1 Independent Actor-Critic](i-learning-to-collaborate/3.-counterfactual-multi-agent-policy-gradients/3.4-methods/3.4.1-independent-actor-critic.md)
    * [3.4.2 Counterfactual Multi-Agent Policy Gradients](i-learning-to-collaborate/3.-counterfactual-multi-agent-policy-gradients/3.4-methods/3.4.2-counterfactual-multi-agent-policy-gradients.md)
    * [3.4.2.1 COMA Algorithm and baseline lemma](i-learning-to-collaborate/3.-counterfactual-multi-agent-policy-gradients/3.4-methods/untitled.md)
  * [3.5 Results](i-learning-to-collaborate/3.-counterfactual-multi-agent-policy-gradients/3.5-results.md)
  * [3.6 Conclusions & Future Work](i-learning-to-collaborate/3.-counterfactual-multi-agent-policy-gradients/3.6-conclusions-and-future-work.md)
* [4 Multi-Agent Common Knowledge Reinforcement Learning](i-learning-to-collaborate/4-multi-agent-common-knowledge-reinforcement-learning/README.md)
  * [4.1 Introduction](i-learning-to-collaborate/4-multi-agent-common-knowledge-reinforcement-learning/4.1-introduction.md)
  * [4.2 Related Work](i-learning-to-collaborate/4-multi-agent-common-knowledge-reinforcement-learning/4.2-related-work.md)
  * [4.3 Dec-POMDP and Features](i-learning-to-collaborate/4-multi-agent-common-knowledge-reinforcement-learning/4.3-dec-pomdp-and-features.md)
  * [4.4 Common Knowledge](i-learning-to-collaborate/4-multi-agent-common-knowledge-reinforcement-learning/4.4-common-knowledge.md)
  * [4.5 Multi-Agent Common Knowledge Reinforcement Learning](i-learning-to-collaborate/4-multi-agent-common-knowledge-reinforcement-learning/4.5-multi-agent-common-knowledge-reinforcement-learning.md)
  * [4.6 Pairwise MACKRL](i-learning-to-collaborate/4-multi-agent-common-knowledge-reinforcement-learning/4.6-pairwise-mackrl.md)
  * [4.7 Experiments and Results](i-learning-to-collaborate/4-multi-agent-common-knowledge-reinforcement-learning/4.7-experiments-and-results.md)
  * [4.8 Conclusion & Future Work](i-learning-to-collaborate/4-multi-agent-common-knowledge-reinforcement-learning/4.8-conclusion-and-future-work.md)
* [5 Stabilising Experience Replay](i-learning-to-collaborate/5-stabilising-experience-replay/README.md)
  * [5.1 Introduction](i-learning-to-collaborate/5-stabilising-experience-replay/5.1-introduction.md)
  * [5.2 Related Work](i-learning-to-collaborate/5-stabilising-experience-replay/5.2-related-work.md)
  * [5.3 Methods](i-learning-to-collaborate/5-stabilising-experience-replay/5.3-methods/README.md)
    * [5.3.1 Multi-Agent Importance Sampling](i-learning-to-collaborate/5-stabilising-experience-replay/5.3-methods/5.3.1-multi-agent-importance-sampling.md)
    * [5.3.2 Multi-Agent Fingerprints](i-learning-to-collaborate/5-stabilising-experience-replay/5.3-methods/5.3.2-multi-agent-fingerprints.md)
  * [5.4 Experiments](i-learning-to-collaborate/5-stabilising-experience-replay/5.4-experiments/README.md)
    * [5.4.1 Architecture](i-learning-to-collaborate/5-stabilising-experience-replay/5.4-experiments/5.4.1-architecture.md)
  * [5.5 Results](i-learning-to-collaborate/5-stabilising-experience-replay/5.5-results/README.md)
    * [5.5.1 Importance Sampling](i-learning-to-collaborate/5-stabilising-experience-replay/5.5-results/5.5.1-importance-sampling.md)
    * [5.5.2 Fingerprints](i-learning-to-collaborate/5-stabilising-experience-replay/5.5-results/5.5.2-fingerprints.md)
    * [5.5.3 Informative Trajectories](i-learning-to-collaborate/5-stabilising-experience-replay/5.5-results/5.5.3-informative-trajectories.md)
  * [5.6 Conclusion & Future Work](i-learning-to-collaborate/5-stabilising-experience-replay/5.6-conclusion-and-future-work.md)

## II Learning to Communicate

* [6. Learning to Communicate with Deep Multi-Agent ReinforcementLearning](ii-learning-to-communicate/6-learning-to-communicate-with-deep-multi-agent-reinforcementlearning/README.md)
  * [6.1 Introduction](ii-learning-to-communicate/6-learning-to-communicate-with-deep-multi-agent-reinforcementlearning/6.1-introduction.md)
  * [6.2 Related Work](ii-learning-to-communicate/6-learning-to-communicate-with-deep-multi-agent-reinforcementlearning/6.2-related-work.md)
  * [6.3 Setting](ii-learning-to-communicate/6-learning-to-communicate-with-deep-multi-agent-reinforcementlearning/6.3-setting.md)
  * [6.4 Methods](ii-learning-to-communicate/6-learning-to-communicate-with-deep-multi-agent-reinforcementlearning/6.4-methods/README.md)
    * [6.4.1 Reinforced Inter-Agent Learning](ii-learning-to-communicate/6-learning-to-communicate-with-deep-multi-agent-reinforcementlearning/6.4-methods/6.4.1-reinforced-inter-agent-learning.md)
    * [6.4.2 Differentiable Inter-Agent Learning](ii-learning-to-communicate/6-learning-to-communicate-with-deep-multi-agent-reinforcementlearning/6.4-methods/6.4.2-differentiable-inter-agent-learning.md)
  * [6.5 DIAL Details](ii-learning-to-communicate/6-learning-to-communicate-with-deep-multi-agent-reinforcementlearning/6.5-dial-details.md)
  * [6.6 Experiments](ii-learning-to-communicate/6-learning-to-communicate-with-deep-multi-agent-reinforcementlearning/6.6-experiments/README.md)
    * [6.6.1 Model Architecture](ii-learning-to-communicate/6-learning-to-communicate-with-deep-multi-agent-reinforcementlearning/6.6-experiments/6.6.1-model-architecture.md)
    * [6.6.2 Switch Riddle](ii-learning-to-communicate/6-learning-to-communicate-with-deep-multi-agent-reinforcementlearning/6.6-experiments/6.6.2-switch-riddle.md)
    * [6.6.3 MNIST Games](ii-learning-to-communicate/6-learning-to-communicate-with-deep-multi-agent-reinforcementlearning/6.6-experiments/6.6.3-mnist-games.md)
    * [6.6.4 Effect of Channel Noise](ii-learning-to-communicate/6-learning-to-communicate-with-deep-multi-agent-reinforcementlearning/6.6-experiments/6.6.4-effect-of-channel-noise.md)
  * [6.7 Conclusion & Future Work](ii-learning-to-communicate/6-learning-to-communicate-with-deep-multi-agent-reinforcementlearning/6.7-conclusion-and-future-work.md)
* [7. Bayesian Action Decoder](ii-learning-to-communicate/7-bayesian-action-decoder/README.md)
  * [7.1 Introduction](ii-learning-to-communicate/7-bayesian-action-decoder/7.1-introduction.md)
  * [7.2 Setting](ii-learning-to-communicate/7-bayesian-action-decoder/7.2-setting.md)
  * [7.3 Method](ii-learning-to-communicate/7-bayesian-action-decoder/7.3-method/README.md)
    * [7.3.1 Public belief](ii-learning-to-communicate/7-bayesian-action-decoder/7.3-method/7.3.1-public-belief.md)
    * [7.3.2 Public Belief MDP](ii-learning-to-communicate/7-bayesian-action-decoder/7.3-method/7.3.2-public-belief-mdp.md)
    * [7.3.3 Sampling Deterministic Partial Policies](ii-learning-to-communicate/7-bayesian-action-decoder/7.3-method/7.3.3-sampling-deterministic-partial-policies.md)
    * [7.3.4 Factorised Belief Updates](ii-learning-to-communicate/7-bayesian-action-decoder/7.3-method/7.3.4-factorised-belief-updates.md)
    * [7.3.5 Self-Consistent Beliefs](ii-learning-to-communicate/7-bayesian-action-decoder/7.3-method/7.3.5-self-consistent-beliefs.md)
  * [7.4 Experiments and Results](ii-learning-to-communicate/7-bayesian-action-decoder/7.4-experiments-and-results/README.md)
    * [7.4.1 Matrix Game](ii-learning-to-communicate/7-bayesian-action-decoder/7.4-experiments-and-results/7.4.1-matrix-game.md)
    * [7.4.2 Hanabi](ii-learning-to-communicate/7-bayesian-action-decoder/7.4-experiments-and-results/7.4.2-hanabi.md)
    * [7.4.3 Observations and Actions](ii-learning-to-communicate/7-bayesian-action-decoder/7.4-experiments-and-results/7.4.3-observations-and-actions.md)
    * [7.4.4 Beliefs in Hanabi](ii-learning-to-communicate/7-bayesian-action-decoder/7.4-experiments-and-results/7.4.4-beliefs-in-hanabi.md)
    * [7.4.5 Architecture Details for Baselines and Method](ii-learning-to-communicate/7-bayesian-action-decoder/7.4-experiments-and-results/7.4.5-architecture-details-for-baselines-and-method.md)
    * [7.4.6 Hyperparamters](ii-learning-to-communicate/7-bayesian-action-decoder/7.4-experiments-and-results/7.4.6-hyperparamters.md)
    * [7.4.7 Results on Hanabi](ii-learning-to-communicate/7-bayesian-action-decoder/7.4-experiments-and-results/7.4.7-results-on-hanabi.md)
  * [7.5 Related Work](ii-learning-to-communicate/7-bayesian-action-decoder/7.5-related-work/README.md)
    * [7.5.1 Learning to Communicate](ii-learning-to-communicate/7-bayesian-action-decoder/7.5-related-work/7.5.1-learning-to-communicate.md)
    * [7.5.2 Research on Hanabi](ii-learning-to-communicate/7-bayesian-action-decoder/7.5-related-work/7.5.2-research-on-hanabi.md)
    * [7.5.3 Belief State Methods](ii-learning-to-communicate/7-bayesian-action-decoder/7.5-related-work/7.5.3-belief-state-methods.md)
  * [7.6 Conclusion & Future Work](ii-learning-to-communicate/7-bayesian-action-decoder/7.6-conclusion-and-future-work.md)

## III Learning to Reciprocate

* [8. Learning with Opponent-Learning Awareness](iii-learning-to-reciprocate/8-learning-with-opponent-learning-awareness/README.md)
  * [8.1 Introduction](iii-learning-to-reciprocate/8-learning-with-opponent-learning-awareness/8.1-introduction.md)
  * [8.2 Related Work](iii-learning-to-reciprocate/8-learning-with-opponent-learning-awareness/8.2-related-work.md)
  * [8.3 Methods](iii-learning-to-reciprocate/8-learning-with-opponent-learning-awareness/8.3-methods.md)
  * [8.4 Experimental Setup](iii-learning-to-reciprocate/8-learning-with-opponent-learning-awareness/8.4-experimental-setup.md)
  * [8.5 Results](iii-learning-to-reciprocate/8-learning-with-opponent-learning-awareness/8.5-results.md)
  * [8.6 Conclusion & Future Work](iii-learning-to-reciprocate/8-learning-with-opponent-learning-awareness/8.6-conclusion-and-future-work.md)
* [9. DiCE: The Infinitely Differentiable Monte Carlo Estimator](iii-learning-to-reciprocate/9-dice-the-infinitely-differentiable-monte-carlo-estimator/README.md)
  * [9.1 Introduction](iii-learning-to-reciprocate/9-dice-the-infinitely-differentiable-monte-carlo-estimator/9.1-introduction.md)
  * [9.2 Background](iii-learning-to-reciprocate/9-dice-the-infinitely-differentiable-monte-carlo-estimator/9.2-background.md)
  * [9.3 Higher Order Gradients](iii-learning-to-reciprocate/9-dice-the-infinitely-differentiable-monte-carlo-estimator/9.3-higher-order-gradients.md)
  * [9.4 Correct Gradient Estimators with DiCE](iii-learning-to-reciprocate/9-dice-the-infinitely-differentiable-monte-carlo-estimator/9.4-correct-gradient-estimators-with-dice.md)
  * [9.5 Case Studies](iii-learning-to-reciprocate/9-dice-the-infinitely-differentiable-monte-carlo-estimator/9.5-case-studies.md)
  * [9.6 Related Work](iii-learning-to-reciprocate/9-dice-the-infinitely-differentiable-monte-carlo-estimator/9.6-related-work.md)
  * [9.7 Conclusion & Future Work](iii-learning-to-reciprocate/9-dice-the-infinitely-differentiable-monte-carlo-estimator/9.7-conclusion-and-future-work.md)

## Reference

* [Reference](reference/reference.md)

