# Table of contents

* [Deep Multi-Agent Reinforcement Learning](README.md)

## Abstract & Contents

* [Abstract](abstract-and-contents/deep-multi-agent-reinforcement-learning.md)

## 1. Introduction

* [1. INTRODUCTION](1.-introduction/1.-introduction/README.md)
  * [1.1 The Industrial Revolution, Cognition, and Computers](1.-introduction/1.-introduction/1.1-the-industrial-revolution-cognition-and-computers.md)
  * [1.2 Deep Multi-Agent Reinforcement-Learning](1.-introduction/1.-introduction/1.2-deep-multi-agent-reinforcement-learning.md)
  * [1.3  Overall Structure](1.-introduction/1.-introduction/1.3-overall-structure.md)

## 2. Background

* [2. BACKGROUND](2.-background/2.-background/README.md)
  * [2.1 Reinforcement Learning](2.-background/2.-background/2.1-reinforcement-learning.md)
  * [2.2 Multi-Agent Settings](2.-background/2.-background/2.2-multi-agent-settings.md)
  * [2.3 Centralized vs Decentralized Control](2.-background/2.-background/2.3-centralised-vs-decentralised-control.md)
  * [2.4  Cooperative, Zero-sum, and General-Sum](2.-background/2.-background/2.4-cooperative-zero-sum-and-general-sum.md)
  * [2.5 Partial Observability](2.-background/2.-background/2.5-partial-observability.md)
  * [2.6 Centralized Training, Decentralized Execution](2.-background/2.-background/2.6-centralised-training-decentralised-execution.md)
  * [2.7 Value Functions](2.-background/2.-background/2.7-value-functions.md)
  * [2.8 Nash Equilibria](2.-background/2.-background/2.8-nash-equilibria.md)
  * [2.9 Deep Learning for MARL](2.-background/2.-background/2.9-deep-learning-for-marl.md)
  * [2.10 Q-Learning and DQN](2.-background/2.-background/2.10-q-learning-and-dqn.md)
  * [2.11 Reinforce and Actor-Critic](2.-background/2.-background/2.11-reinforce-and-actor-critic.md)

## I Learning to Collaborate

* [3. Counterfactual Multi-Agent Policy Gradients](i-learning-to-collaborate/3.-counterfactual-multi-agent-policy-gradients/README.md)
  * [3.1 Introduction](i-learning-to-collaborate/3.-counterfactual-multi-agent-policy-gradients/3.1-introduction.md)
  * [3.2 Related Work](i-learning-to-collaborate/3.-counterfactual-multi-agent-policy-gradients/3.2-related-work.md)
  * [3.3 Multi-Agent StarCraft Micromanagement](i-learning-to-collaborate/3.-counterfactual-multi-agent-policy-gradients/3.3-multi-agent-starcraft-micromanagement.md)
  * [3.4 Methods](i-learning-to-collaborate/3.-counterfactual-multi-agent-policy-gradients/3.4-methods/README.md)
    * [3.4.1 Independent Actor-Critic](i-learning-to-collaborate/3.-counterfactual-multi-agent-policy-gradients/3.4-methods/3.4.1-independent-actor-critic.md)
    * [3.4.2 Counterfactual Multi-Agent Policy Gradients](i-learning-to-collaborate/3.-counterfactual-multi-agent-policy-gradients/3.4-methods/3.4.2-counterfactual-multi-agent-policy-gradients.md)
    * [3.4.2.1 baseline lemma](i-learning-to-collaborate/3.-counterfactual-multi-agent-policy-gradients/3.4-methods/untitled.md)
    * [3.4.2.2 COMA Algorithm](i-learning-to-collaborate/3.-counterfactual-multi-agent-policy-gradients/3.4-methods/3.4.2.2-coma-algorithm.md)
  * [3.5 Results](i-learning-to-collaborate/3.-counterfactual-multi-agent-policy-gradients/3.5-results.md)
  * [3.6 Conclusions & Future Work](i-learning-to-collaborate/3.-counterfactual-multi-agent-policy-gradients/3.6-conclusions-and-future-work.md)
* [4 Multi-Agent Common Knowledge Reinforcement Learning](i-learning-to-collaborate/4-multi-agent-common-knowledge-reinforcement-learning/README.md)
  * [4.1 Introduction](i-learning-to-collaborate/4-multi-agent-common-knowledge-reinforcement-learning/4.1-introduction.md)
  * [4.2 Related Work](i-learning-to-collaborate/4-multi-agent-common-knowledge-reinforcement-learning/4.2-related-work.md)
  * [4.3 Dec-POMDP and Features](i-learning-to-collaborate/4-multi-agent-common-knowledge-reinforcement-learning/4.3-dec-pomdp-and-features.md)
  * [4.4 Common Knowledge](i-learning-to-collaborate/4-multi-agent-common-knowledge-reinforcement-learning/4.4-common-knowledge.md)
  * [4.5 Multi-Agent Common Knowledge Reinforcement Learning](i-learning-to-collaborate/4-multi-agent-common-knowledge-reinforcement-learning/4.5-multi-agent-common-knowledge-reinforcement-learning.md)
  * [4.6 Pairwise MACKRL](i-learning-to-collaborate/4-multi-agent-common-knowledge-reinforcement-learning/4.6-pairwise-mackrl.md)
  * [4.7 Experiments and Results](i-learning-to-collaborate/4-multi-agent-common-knowledge-reinforcement-learning/4.7-experiments-and-results.md)
  * [4.8 Conclusion & Future Work](i-learning-to-collaborate/4-multi-agent-common-knowledge-reinforcement-learning/4.8-conclusion-and-future-work.md)
* [5 Stabilizing Experience Replay](i-learning-to-collaborate/5-stabilising-experience-replay/README.md)
  * [5.1 Introduction](i-learning-to-collaborate/5-stabilising-experience-replay/5.1-introduction.md)
  * [5.2 Related Work](i-learning-to-collaborate/5-stabilising-experience-replay/5.2-related-work.md)
  * [5.3 Methods](i-learning-to-collaborate/5-stabilising-experience-replay/5.3-methods/README.md)
    * [5.3.1 Multi-Agent Importance Sampling](i-learning-to-collaborate/5-stabilising-experience-replay/5.3-methods/5.3.1-multi-agent-importance-sampling.md)
    * [5.3.2 Multi-Agent Fingerprints](i-learning-to-collaborate/5-stabilising-experience-replay/5.3-methods/5.3.2-multi-agent-fingerprints.md)
  * [5.4 Experiments](i-learning-to-collaborate/5-stabilising-experience-replay/5.4-experiments/README.md)
    * [5.4.1 Architecture](i-learning-to-collaborate/5-stabilising-experience-replay/5.4-experiments/5.4.1-architecture.md)
  * [5.5 Results](i-learning-to-collaborate/5-stabilising-experience-replay/5.5-results/README.md)
    * [5.5.1 Importance Sampling](i-learning-to-collaborate/5-stabilising-experience-replay/5.5-results/5.5.1-importance-sampling.md)
    * [5.5.2 Fingerprints](i-learning-to-collaborate/5-stabilising-experience-replay/5.5-results/5.5.2-fingerprints.md)
    * [5.5.3 Informative Trajectories](i-learning-to-collaborate/5-stabilising-experience-replay/5.5-results/5.5.3-informative-trajectories.md)
  * [5.6 Conclusion & Future Work](i-learning-to-collaborate/5-stabilising-experience-replay/5.6-conclusion-and-future-work.md)

## II Learning to Communicate

* [6. Learning to Communicate with Deep Multi-Agent ReinforcementLearning](ii-learning-to-communicate/6-learning-to-communicate-with-deep-multi-agent-reinforcementlearning/README.md)
  * [6.1 Introduction](ii-learning-to-communicate/6-learning-to-communicate-with-deep-multi-agent-reinforcementlearning/6.1-introduction.md)
  * [6.2 Related Work](ii-learning-to-communicate/6-learning-to-communicate-with-deep-multi-agent-reinforcementlearning/6.2-related-work.md)
  * [6.3 Setting](ii-learning-to-communicate/6-learning-to-communicate-with-deep-multi-agent-reinforcementlearning/6.3-setting.md)
  * [6.4 Methods](ii-learning-to-communicate/6-learning-to-communicate-with-deep-multi-agent-reinforcementlearning/6.4-methods/README.md)
    * [6.4.1 Reinforced Inter-Agent Learning](ii-learning-to-communicate/6-learning-to-communicate-with-deep-multi-agent-reinforcementlearning/6.4-methods/6.4.1-reinforced-inter-agent-learning.md)
    * [6.4.2 Differentiable Inter-Agent Learning](ii-learning-to-communicate/6-learning-to-communicate-with-deep-multi-agent-reinforcementlearning/6.4-methods/6.4.2-differentiable-inter-agent-learning.md)
  * [6.5 DIAL Details](ii-learning-to-communicate/6-learning-to-communicate-with-deep-multi-agent-reinforcementlearning/6.5-dial-details.md)
  * [6.6 Experiments](ii-learning-to-communicate/6-learning-to-communicate-with-deep-multi-agent-reinforcementlearning/6.6-experiments/README.md)
    * [6.6.1 Model Architecture](ii-learning-to-communicate/6-learning-to-communicate-with-deep-multi-agent-reinforcementlearning/6.6-experiments/6.6.1-model-architecture.md)
    * [6.6.2 Switch Riddle](ii-learning-to-communicate/6-learning-to-communicate-with-deep-multi-agent-reinforcementlearning/6.6-experiments/6.6.2-switch-riddle.md)
    * [6.6.3 MNIST Games](ii-learning-to-communicate/6-learning-to-communicate-with-deep-multi-agent-reinforcementlearning/6.6-experiments/6.6.3-mnist-games.md)
    * [6.6.4 Effect of Channel Noise](ii-learning-to-communicate/6-learning-to-communicate-with-deep-multi-agent-reinforcementlearning/6.6-experiments/6.6.4-effect-of-channel-noise.md)
  * [6.7 Conclusion & Future Work](ii-learning-to-communicate/6-learning-to-communicate-with-deep-multi-agent-reinforcementlearning/6.7-conclusion-and-future-work.md)
* [7. Bayesian Action Decoder](ii-learning-to-communicate/7-bayesian-action-decoder/README.md)
  * [7.1 Introduction](ii-learning-to-communicate/7-bayesian-action-decoder/7.1-introduction.md)
  * [7.2 Setting](ii-learning-to-communicate/7-bayesian-action-decoder/7.2-setting.md)
  * [7.3 Method](ii-learning-to-communicate/7-bayesian-action-decoder/7.3-method/README.md)
    * [7.3.1 Public belief](ii-learning-to-communicate/7-bayesian-action-decoder/7.3-method/7.3.1-public-belief.md)
    * [7.3.2 Public Belief MDP](ii-learning-to-communicate/7-bayesian-action-decoder/7.3-method/7.3.2-public-belief-mdp.md)
    * [7.3.3 Sampling Deterministic Partial Policies](ii-learning-to-communicate/7-bayesian-action-decoder/7.3-method/7.3.3-sampling-deterministic-partial-policies.md)
    * [7.3.4 Factorised Belief Updates](ii-learning-to-communicate/7-bayesian-action-decoder/7.3-method/7.3.4-factorised-belief-updates.md)
    * [7.3.5 Self-Consistent Beliefs](ii-learning-to-communicate/7-bayesian-action-decoder/7.3-method/7.3.5-self-consistent-beliefs.md)
  * [7.4 Experiments and Results](ii-learning-to-communicate/7-bayesian-action-decoder/7.4-experiments-and-results/README.md)
    * [7.4.1 Matrix Game](ii-learning-to-communicate/7-bayesian-action-decoder/7.4-experiments-and-results/7.4.1-matrix-game.md)
    * [7.4.2 Hanabi](ii-learning-to-communicate/7-bayesian-action-decoder/7.4-experiments-and-results/7.4.2-hanabi.md)
    * [7.4.3 Observations and Actions](ii-learning-to-communicate/7-bayesian-action-decoder/7.4-experiments-and-results/7.4.3-observations-and-actions.md)
    * [7.4.4 Beliefs in Hanabi](ii-learning-to-communicate/7-bayesian-action-decoder/7.4-experiments-and-results/7.4.4-beliefs-in-hanabi.md)
    * [7.4.5 Architecture Details for Baselines and Method](ii-learning-to-communicate/7-bayesian-action-decoder/7.4-experiments-and-results/7.4.5-architecture-details-for-baselines-and-method.md)
    * [7.4.6 Hyperparamters](ii-learning-to-communicate/7-bayesian-action-decoder/7.4-experiments-and-results/7.4.6-hyperparamters.md)
    * [7.4.7 Results on Hanabi](ii-learning-to-communicate/7-bayesian-action-decoder/7.4-experiments-and-results/7.4.7-results-on-hanabi.md)
  * [7.5 Related Work](ii-learning-to-communicate/7-bayesian-action-decoder/7.5-related-work/README.md)
    * [7.5.1 Learning to Communicate](ii-learning-to-communicate/7-bayesian-action-decoder/7.5-related-work/7.5.1-learning-to-communicate.md)
    * [7.5.2 Research on Hanabi](ii-learning-to-communicate/7-bayesian-action-decoder/7.5-related-work/7.5.2-research-on-hanabi.md)
    * [7.5.3 Belief State Methods](ii-learning-to-communicate/7-bayesian-action-decoder/7.5-related-work/7.5.3-belief-state-methods.md)
  * [7.6 Conclusion & Future Work](ii-learning-to-communicate/7-bayesian-action-decoder/7.6-conclusion-and-future-work.md)

## III Learning to Reciprocate

* [8. Learning with Opponent-Learning Awareness](iii-learning-to-reciprocate/8-learning-with-opponent-learning-awareness/README.md)
  * [8.1 Introduction](iii-learning-to-reciprocate/8-learning-with-opponent-learning-awareness/8.1-introduction.md)
  * [8.2 Related Work](iii-learning-to-reciprocate/8-learning-with-opponent-learning-awareness/8.2-related-work.md)
  * [8.3 Methods](iii-learning-to-reciprocate/8-learning-with-opponent-learning-awareness/8.3-methods/README.md)
    * [8.3.1 Naive Learner](iii-learning-to-reciprocate/8-learning-with-opponent-learning-awareness/8.3-methods/8.3.1-naive-learner.md)
    * [8.3.2 Learning with Opponent Learning Awareness](iii-learning-to-reciprocate/8-learning-with-opponent-learning-awareness/8.3-methods/8.3.2-learning-with-opponent-learning-awareness.md)
    * [8.3.3. Learning via Policy gradient](iii-learning-to-reciprocate/8-learning-with-opponent-learning-awareness/8.3-methods/8.3.3.-learning-via-policy-gradient.md)
    * [8.3.4 LOLA with Opponent modeling](iii-learning-to-reciprocate/8-learning-with-opponent-learning-awareness/8.3-methods/8.3.4-lola-with-opponent-modeling.md)
    * [8.3.5 Higher-Order LOLA](iii-learning-to-reciprocate/8-learning-with-opponent-learning-awareness/8.3-methods/8.3.5-higher-order-lola.md)
  * [8.4 Experimental Setup](iii-learning-to-reciprocate/8-learning-with-opponent-learning-awareness/8.4-experimental-setup/README.md)
    * [8.4.1 Iterated Games](iii-learning-to-reciprocate/8-learning-with-opponent-learning-awareness/8.4-experimental-setup/8.4.1-iterated-games.md)
    * [8.4.2 Coin Game](iii-learning-to-reciprocate/8-learning-with-opponent-learning-awareness/8.4-experimental-setup/8.4.2-coin-game.md)
    * [8.4.3 Training Details](iii-learning-to-reciprocate/8-learning-with-opponent-learning-awareness/8.4-experimental-setup/8.4.3-training-details.md)
  * [8.5 Results](iii-learning-to-reciprocate/8-learning-with-opponent-learning-awareness/8.5-results/README.md)
    * [8.5.1 Iterated Games](iii-learning-to-reciprocate/8-learning-with-opponent-learning-awareness/8.5-results/8.5.1-iterated-games.md)
    * [8.5.2 Coin Game](iii-learning-to-reciprocate/8-learning-with-opponent-learning-awareness/8.5-results/8.5.2-coin-game.md)
    * [8.5.3 Exploitability of LOLA](iii-learning-to-reciprocate/8-learning-with-opponent-learning-awareness/8.5-results/8.5.3-exploitability-of-lola.md)
  * [8.6 Conclusion & Future Work](iii-learning-to-reciprocate/8-learning-with-opponent-learning-awareness/8.6-conclusion-and-future-work.md)
* [9. DiCE: The Infinitely Differentiable Monte Carlo Estimator](iii-learning-to-reciprocate/9-dice-the-infinitely-differentiable-monte-carlo-estimator/README.md)
  * [9.1 Introduction](iii-learning-to-reciprocate/9-dice-the-infinitely-differentiable-monte-carlo-estimator/9.1-introduction.md)
  * [9.2 Background](iii-learning-to-reciprocate/9-dice-the-infinitely-differentiable-monte-carlo-estimator/9.2-background/README.md)
    * [9.2.1 Stochastic Computation Graphs](iii-learning-to-reciprocate/9-dice-the-infinitely-differentiable-monte-carlo-estimator/9.2-background/9.2.1-stochastic-computation-graphs.md)
    * [9.2.2 Surrogate Losses](iii-learning-to-reciprocate/9-dice-the-infinitely-differentiable-monte-carlo-estimator/9.2-background/9.2.2-surrogate-losses.md)
  * [9.3 Higher Order Gradients](iii-learning-to-reciprocate/9-dice-the-infinitely-differentiable-monte-carlo-estimator/9.3-higher-order-gradients/README.md)
    * [9.3.1 Higher Order Gradient Estimators](iii-learning-to-reciprocate/9-dice-the-infinitely-differentiable-monte-carlo-estimator/9.3-higher-order-gradients/9.3.1-higher-order-gradient-estimators.md)
    * [9.3.2 Higher Order Surrogate Losses](iii-learning-to-reciprocate/9-dice-the-infinitely-differentiable-monte-carlo-estimator/9.3-higher-order-gradients/9.3.2-higher-order-surrogate-losses.md)
    * [9.3.3. Simple Failing Example](iii-learning-to-reciprocate/9-dice-the-infinitely-differentiable-monte-carlo-estimator/9.3-higher-order-gradients/9.3.3.-simple-failing-example.md)
  * [9.4 Correct Gradient Estimators with DiCE](iii-learning-to-reciprocate/9-dice-the-infinitely-differentiable-monte-carlo-estimator/9.4-correct-gradient-estimators-with-dice/README.md)
    * [9.4.1 Implement of DiCE](iii-learning-to-reciprocate/9-dice-the-infinitely-differentiable-monte-carlo-estimator/9.4-correct-gradient-estimators-with-dice/9.4.1-implement-of-dice.md)
    * [9.4.2 Casuality](iii-learning-to-reciprocate/9-dice-the-infinitely-differentiable-monte-carlo-estimator/9.4-correct-gradient-estimators-with-dice/9.4.2-casuality.md)
    * [9.4.3 First Order Variance Reduction](iii-learning-to-reciprocate/9-dice-the-infinitely-differentiable-monte-carlo-estimator/9.4-correct-gradient-estimators-with-dice/9.4.3-first-order-variance-reduction.md)
    * [9.4.4 Hessian-Vector Product](iii-learning-to-reciprocate/9-dice-the-infinitely-differentiable-monte-carlo-estimator/9.4-correct-gradient-estimators-with-dice/9.4.4-hessian-vector-product.md)
  * [9.5 Case Studies](iii-learning-to-reciprocate/9-dice-the-infinitely-differentiable-monte-carlo-estimator/9.5-case-studies/README.md)
    * [9.5.1 Empirical Verification](iii-learning-to-reciprocate/9-dice-the-infinitely-differentiable-monte-carlo-estimator/9.5-case-studies/9.5.1-empirical-verification.md)
    * [9.5.2 DiCE For multi-agent RL](iii-learning-to-reciprocate/9-dice-the-infinitely-differentiable-monte-carlo-estimator/9.5-case-studies/9.5.2-dice-for-multi-agent-rl.md)
  * [9.6 Related Work](iii-learning-to-reciprocate/9-dice-the-infinitely-differentiable-monte-carlo-estimator/9.6-related-work.md)
  * [9.7 Conclusion & Future Work](iii-learning-to-reciprocate/9-dice-the-infinitely-differentiable-monte-carlo-estimator/9.7-conclusion-and-future-work.md)

## Reference

* [Reference](reference/reference.md)

## After

* [역자 후기](after/undefined.md)

