# 2.7 Value Functions

Multi agent환경은 많은 요소들에서 joint action에 대한 신경을 써야합니다. 이번장에서는 multi agent환경에서 바뀐 value function, action-value function, objective function등에 대해 알아볼껀데, 한 agent에 대한 value function을 먼저 정의해 보겠습니다.

                                       $$ V^a_\bold{\pi}(s_t) = \mathbb{E}_{s_{t+1:\infty},\bold{u}_{t:\infty}} [R^a_t|s_t] (\because R^a_t(\tau) = \sum^\infty_{t'=t}{\gamma^{t'-t}r^a_{t'}})$$

이는 policy에 의한 state distribution과 joint action에 관한 기댓값입니다. 또한 action-value function은

                                                     $$ Q^a_\bold{\pi}(s_t,\bold{u}_t) = \mathbb{E}_{s_{t+1}:\infty,\bold{u}_{t+1}:\infty}[R^a_t|s_t,\bold{u}_t] $$

 로 나타낼 수 있습니다. 우리가 maximize하기 위한 objective는

                                                               $$ J^a(\bold{\pi}) = \mathbb{E}_{s0:\infty,\bold{u}_0:\infty}[R^at] $$

다음처럼 나타낼 수 있습니다. 이 뒤로는 모호하지않을 때, notation일부를 생략할 수 있는데, 잘 유의하기를 바랍니다.

