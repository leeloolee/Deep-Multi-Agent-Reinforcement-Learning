# 2.2 Multi-Agent Settings

이제 MARL에서의 formalization을 알아보겠습니다.

* $$ G = <S,U,P,r,Z,O,n,\gamma>$$
  * $$ S $$: n개의 Agent들이 놓여있는 state에 대한 vector입니다.
  * $$U$$: n개의 Agent들이 고른 action에 대한 vector입니다.
  * $$P$$: n개의 Agent들에 의한 joint action $$\bold{u}$$에 대한 각각의 agent 시점 $$t$$에서의 state $$s_t$$가 next state $$s_{t+1} $$가 될 확률에 대한 vector입니다.
    * 한 agent의 action $$u^a \in U$$에 대해, n개의 agent에 대한 joint action은 $$ \bold{u} \in \bold{U} \equiv{U}^n $$로 나타낼 수 있습니다.
  * $$r$$: 전체 agent에 대한 state $$S$$에서 joint action $$\bold{U}$$사이의 한 agent의 action $$a$$로 인해 받는 reward 에 대한 scalar입니다.
    * $$r(s,\bold{u},a) : S\times \bold{U} \times A \rightarrow \mathbb{R} $$로 나타낼 수 있습니다.
  * $$Z$$: n개의 Agent의 observation 전체에 대한 vector입니다. 어느 시점 $$t$$에서의 한 agent에 대한 observation $$ o^a_t $$가 있을 때, 이는 모두 $$o^a_t \in Z $$로 나타낼 수 있습니다.
  * $$O$$: n개의 Agent의 observation function vector입니다. 모든 agent에 대해 어느 state에 존재할 때, observate가능한 영역은 다음과 같이 나타낼 수 있습니다. $$O(s,a) : S \times A \rightarrow Z $$
  * $$n$$: Agent의 개수입니다. 이때 Agent $$a $$는 전체 actor에 대한 집합$$A$$에 속하게 됩니다. $$a \in A \equiv \{1,...,n\} $$
  * $$\gamma $$: discount factor 입니다.



