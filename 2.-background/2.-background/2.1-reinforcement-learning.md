# 2.1 Reinforcement Learning

RL에서는 agent가 environment $$ E $$와 계속해서 상호작용합니다. 이때 환경은 MDP를 가정하고서 진행된다면, transition probability function$$ P(s_{t+1}|s_t,u_t)$$ 와 reward function $$ R(s_t,u_t) $$, action space $$U$$ discount factor $$ \gamma $$로 이루어집니다. 

* transition probability function은 $$ P(s_{t+1}|s_t,u_t) : S \times U \times S \rightarrow\mathbb{R}$$ 로, 시점 $$ t $$에서 현재 state $$s_t$$와 그에 따라 agent가 행동할 수 있는 action $$u_t$$, 그로 인해 변하는 다음 next state $$ s_{t+1}$$ 의 등장 확률을 나타내는 function입니다.
* reward function은 $$R(s_t,y_t):S\times U \rightarrow \mathbb{R}$$로, 시점 $$t$$에서 현재 state $$s_t$$와 그에 따른 agent의 행동 action $$u_t$$에 의해 environment $$E$$가 주는 reward에 대한 function입니다.
* discount factor $$ \gamma $$는  미래의 reward에 대한 중요도를 조정하는\(주로 낮게\) 상수입니다.

주로 RL의 objective function은 total expected discounted return per episode를 최대화 하는 것입니다. 이는 수식으로 다음과 같습니다.

                                                                          $$J = \mathbb{E}_{\tau \sim P(\tau)}R_0(\tau)$$

* total discounted return 은$$ R_t(\tau) = \sum_{t'=t}\gamma^{t'-t}r_{t'}$$임을 알 수 있습니다.
* 그리고 그 trajectory가 reward function, policy에 의한 action selection, transition probability function 을 따를 때에 대한 expectation이라면 total expected discounted return per episode를 만족합니다.$$P(\tau) = P(s_0)\prod ^{t=T}_{t=0}P(r_t|s_{t+1},s_t,u_t)P(u_t|s_t)P(s_{t+1}|s_t,u_t)$$ 



