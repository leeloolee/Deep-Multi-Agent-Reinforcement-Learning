# 2.11 Reinforce and Actor-Critic

이번엔 Policy gradient 계열의 background를 보자면, policy gradient는 Q-learning의 policy가 action을 greedy하게 선택하던 것과는 다르게 gradient ascent를 통해 expected discounted total reward를 maximize하는 방향으로 update를 합니다. 이중 가장 간단한 형태의 formulation은 다음과 같습니다.

                                                    $$ g = \mathbb{E}{s_{0:\infty},u_{0:\infty}}{[\sum^T_{t=0}R_t\nabla_{\theta^\pi}\mathrm{log}\pi(u_t|s_t)]}$$

actor-critic 접근 방식에선 주로 policy를 critic의 ascent 방향으로 update를 합니다.

### Partially Observable Actor-Critic

Actor critic의 partially observable 상황에 적용을보면, DRQN과 비슷하게, recurrent neural network를 도입함으로써 해결이 되었습니다.

### Independent Actor-Critic\(IAC\)

IAC에서도, 각자의 agent가 local observation에 대해 history를 만들고 그것이 IAC network의 input이 됩니다. 그리고 주로 Actor와 critic의 Network parameter 일부가 sharing되는 기법을 사용하였습니다. 앞에서 보았던 IQL과 함께 agent가 주변 환경을 static하게 여기는 특성이 있습니다. 이렇게해서 IQL과 IAC는 naive learning의 대표적인 방법들입니다.



