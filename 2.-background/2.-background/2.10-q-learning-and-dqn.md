# 2.10 Q-Learning and DQN

Bellman optimality Equation을 보면 다음과 같습니다.

                                                   $$ Q^*(s,u) = \mathbb{E}_{s'}[r+\gamma\mathrm{max}_{u'}Q^*(s',u')|s,u]$$

이를 반복적으로 Sampling하여 update하는 것이 Value-based RL의 핵심이고, Q는 1-step이지만 더욱 정교한 Q값으로 optimize됩니다. DQN에서 이 iteration을 다음과 같이 진행합니다. 

                                                    $$\mathcal{L_i}(\theta_i) = \mathbb{E}_{s,u,r,s'}[(y^{DQN}_i-Q(s,u;\theta_i))^2]$$ 

y는 target network로 구한 fix된 정교한 Q에대한 정보이고, 이를 L2 loss로 구성해 이 loss를 minimize하는 것입니다. y의 수식은 다음과 같습니다.

                                                     $$ y^{DQN}_i = r +\gamma \mathrm{max}_{u'}{Q(s',u';\theta^-_i) }$$

### Independent Q-learning

 [Tan이 설명한 Independent Q-Learning\(IQL\)](https://web.media.mit.edu/~cynthiab/Readings/tan-MAS-reinfLearn.pdf)은 Q-learning을 MARL로 확장한 형태인데, 각자의 agent a가 독립적인 $$Q^a$$를 배우게 되고, joint action에 대한 고려없이 자신에 대한 action만을 조건으로 합니다.

 이 때, [Tampuu의 논문](https://arxiv.org/pdf/1511.08779.pdf)에서 이를 DQN과 IQL을 합친 framework을 보이는데, 이 각자의 agent가 각자가 동시에 Q-network $$Q^a(s,u^a;\theta^a_i)$$를 학습합니다. 하지만 각 agent의 학습은 다른 agent에게 environment가 non-stationary하게 바뀌는 것 처럼 다가오기 때문에 수렴에 문제가 생기게 되겠지만, 간단한 two-player pong같은 실험으로는 생각보다 괜찮은 성능을 보였습니다. 

### Deep Recurrent Q-Network

DQN 환경과 IQL환경은 모두 fully observable한 상황을 가정합니다. 하지만, 이후에 partially observable할 때 어떻게 해결하느냐를 두고 [해결한 논문](https://arxiv.org/abs/1507.06527)이 있는데, 이는 environment로부터 온전한 state정보를 받을 수가 없으니, observation을 통해 얻은 Q는 실제 Q와 다를 수 밖에 없음을 설명하고, history를 통해 전체의 state를 이해하는데 도움을 받을 수 있다는 아이디어에 착안하였습니다. 결론적으로 agent의 network에 Recurrent layer를 넣어, 이전의 history에 대한 information을 잘 mapping되도록 하였는데, 해결법은 간단하면서도 큰 기여를 한 논문이라고 생각합니다.

