# 2.5 Partial Observability

 agent가 state를 부분관측할 수 있을 때, 그 observation에 대한 function을 앞에서 정의했듯이 $$ O(s,a):S\times A \rightarrow Z $$임을 알고있을 때, 이에 따른\(partially observable 상황에서의\) agent의 policy를 정의하도록 하겠습니다. 

 stochastic policy $$ \pi^a(u^a|\tau^a):T\times U \rightarrow [0,1] $$은 partially observable했던 history\(trajectory\) $$ \tau^a \in T \equiv (Z \times U)^*$$ 에 의해 정의됨을 볼 수 있습니다.

 이 때, history에 대한 통계량\(POMDP상황에서의 observation information\)을 충분히 mapping해야하는데 이전의 연구들에서 Recurrent neural network 계열이 좋은 성능을 보임을 보았습니다.

 또한 여기서는 reward를 agent가 직접관측하는게 아니라면 observation function에 들어가지 않는다고 판단했습니다.

