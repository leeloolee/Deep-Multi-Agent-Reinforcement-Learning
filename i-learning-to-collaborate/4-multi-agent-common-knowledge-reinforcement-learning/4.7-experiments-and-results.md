# 4.7 Experiments and Results

여기서는 MACKRL를 두가지 task에 대해 실험을 진행하였습니다. 

Matrix 실험은 이후에 설명하도록 하겠습니다.

~~이 중에서 첫번째는 partially observable한 matrix game입니다.여기서의 state는 두가지의 random하게 선택된 bits로 구성되는데, 이는 iid를 따릅니다. 첫번째 비트는 information state로, 두 agent모두 관측가능합니다. 두번째 비트는 agent가 플레이하는 두 가지 일반 폼 게임 중 하나를 선택하고 50%확률로 sampling됩니다. 만약 첫번째 bit가 common knowledge상태에 있다면, P\(common knowledge\)가 일어난 것이고, matrix bit는 상대에게 항상 공개되어 모두의 common knowledge가 됩니다. 반면에 첫번째 bit가 볼수 없는 상황이 된다면, 각 agent는 matrix bit를 50%확률로 보게됩니다.~~ 

![](../../.gitbook/assets/marl_47.png)

 ~~Common knowledge가 항상 공개된다면, MACKRL는 joint-action-learning\(JAL\)과 성능이 같았습니다. 중간 정도의 확률일 땐 MACKRL이 IAC와 JAL을 모두 앞섰고, MACKRL~~

### Starcraft II

두번째 실험은 Starcraft II micromanagement의 MARL환경입니다. 이는 starcraft의 설정과 닮았는데, 3대3 마린전, 2 스토커 3질럿전이 있는데,이전 연구에서 independent learner가 실패하는 것을 보였고, 여기서는 성공하는 것을 봄으로써, MACKRL이 유효하다는 것을 보여줍니다.

Policy의 Neural Network Architecturing에 대한 설명을 진행하는데, 두번째와 세번째 hierarchy controllers는 parameter를 공유합니다. 그러므로, agent index나 index pair에 대한 정보를 agent에 넣어주어야 합니다.

![](../../.gitbook/assets/marl_9.png)

Central-V와의 비교에서, 결론적으로 parameter수는 Central-V가 3배 적긴했지만, 결국엔 더 좋은 성능을 보인건 MACKRL이었습니다.

### Is Pair Controller really well trained to do delegate action?

pair controller가 전략적으로 delegate하는 법을 배운다는 것을 보이기 위해 아래와 같은 그림을 제시합니다. 

![](../../.gitbook/assets/marl_10.png)

이는 주어진 pair controller의 common knowledge안에 있는 적에 수에 따른 delegation action $$ u_d$$에 대한 퍼센트를 나타냈습니다.학습 초기에 pair controller는 드물게 decentralized controller에게 delegate했지만, 학습진행됨에 따라, 더 자주 적당한 적의 수가 common knowledge에 있을 때, delegation하는 법을 배웠습니다. 이는 delegation이 각 agent 개인적인 observation에서의 이점을 가져가면서도, common knowledge가 있을 때 협력하는 법도 배우는 것을 알 수 있습니다.

