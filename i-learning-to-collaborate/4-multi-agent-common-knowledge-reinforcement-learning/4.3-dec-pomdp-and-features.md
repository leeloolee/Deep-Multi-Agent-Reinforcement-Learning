# 4.3 Dec-POMDP and Features

이 chapter에서는 MACKRL에서 문제를 정의할 때 가정하는 decentralised partilally observable Markov decision processes\(Dec-POMDP\) 에서의 정의들에 대해 알아보겠습니다.

Dec-POMDP에서 state $$ s \in S$$는 entities $$ e \in \xi$$으로 이루어져 있습니다. 그래서 다음과 같이 표현합니다. $$ s = \{ s^e |e \in \xi\}$$ 그렇다면, agent또한 관측가능한 entities로 생각할 수 있습니다. $$ a \in \mathcal{A} \subseteq \xi $$. 그 외에도, 적, 장애물, 목표등 모두 entities로 나타낼 수 있습니다.

각 timestep마다, 각 agent는 action을 선택하고 다음과 같이 표기할 수 있습니다.

                                                                                $$ u^a_{env} \in \mathcal{U}^a_{env}(s)$$ 

joint action은 다음과 같이 표기 가능합니다. 

                                                              $$ \bold{u}_{env} = (u^1_{env}, ... ,u^n_{env})\in \mathcal{U}^a_{env}(s)$$

 next state $$ s' \in \mathcal{S}$$일 때, transition probability $$ P(s'|s,\bold{u}_{env})$$와, reward function 은 다음과 같이 정의합니다.$$ r(s,\bold{u}_{env})$$

agent는 partial observability를 가지는데, 각 시간마다 각 agent $$a$$는 observation $$ o^a \in \mathcal{Z} $$를 가지고, state features $$ s^e$$는 agent가 볼 수 있는 모든 entites들을 나타냅니다. 이때 agent $$a$$가 entities $$ e $$를 관찰할 수 있는지에 여부는 다음과 같은 binary mask $$ \mu^a(s^a,s^e)\in\{\top,\bot\}$$를 통해 결정됩니다. agent $$a$$는 항상 자기자신을 관찰할 수 있기 때문에, 다음과 같이 표기할 수 있습니다. $$ \mu^a(s^a,s^a) = \top,\forall a \in \mathcal{A}$$. agent가 볼수 있는 모든 entities는 다음과 같이 정의 합니다. 

                                                                 $$\mathcal{M}^a_s = \{e|\mu^a(s^a,s^e)\} \subseteq \xi $$.

agent의 모든 observation은 deterministic한 observation function $$ O(s,a)$$는 다음과 같이 정의할 수 있습니다.

                                                          $$ o^a = O(s,a) = \{s^e|e\in \mathcal{M}^a_s\} \in \mathcal{Z}$$

agent들의 목표는 expected discount reward의 최대화인데, 이는 다음과 같습니다.                     

                                                           $$ \max R_t = \sum^T_{t'=t}\gamma^{t'-t}r(s_{t'},\bold{u}_{t',env})$$

이때, joint policy $$ \pi(\bold{u}_{env}|s)$$는 독립적인 decentralised policies로 사용할 것이기 때문에 다음과 같이 표기합니다. 이는 자신만의 history를 통해 action을 결정하는 agent라고 해석할 수 있습니다.

                                                                             $$\pi^a(u^a_{env}|\tau^a)$$

또한 agent group $$ \mathcal{G} \subseteq\mathcal{A}$$일 때, joint action space 는 다음과 같이 표기 할 수 있습니다.$$\mathcal{U}^\mathcal{G}_{env} $$

마지막으로 중요한 것은, 위의 정의들이 문제를 단순화하기 위해 state를 entities로 낼 수 있고, observation function이 deterministic하다는 가정을 한 Dec-POMDP의 특별 케이스라는 점입니다. 그렇지않으면, observation으로 인한 noise등이 생기면서 entities에 대응하지않는 state feature가 나오는 등 entities에 세운 common knowledge 방해하기 때문입니다.

