# 4.2 Related Work

MARL의 실험은 주로 작은 environment에서만 이루어졌습니다. MARL에서의 joint action space는 action space를 exponential하게 증가시키기 때문입니다. 

 이전의 실험에서 Guestrin은 agent간의 conditional independence를 이용하는 coordination graphs를 도입해 joint action을 선택하였습니다. Sparse cooperative Q-learning도 joint action에 대해 효과적으로 업데이트하기 위해 coordination graph를 사용하였습니다. 하지만 이는 coordination graph가 알려져 있어, 이를 자유롭게 관찰하거나 communication할 수 있어야만 합니다. 또한, conditional independencies가 없거나, 여전히 large joint action을 가지는 문제점도 남아있습니다.

 Genter는 ad-hoc teamwork\(agent가 이전에 못봤던 non-learning agent 와 만났을 때 어떻게 협력하는지\)를 실험하였습니다. Albrecht는 이 ad hoc teamwork를 상대의 행동에 대한 가설을 parameterized하고, 각 observation마다 업데이트 하는 belief를 만듬으로써 해결하였습니다. Panella는 다른 agent의 행동을 확률적인 process로 보고, 그들에 대한 belief를 만드는 방식으로 해결하였습니다. Makino는 다른 agent들의 policy에 대한 belief를 설명하는 algorithm을 개발하였습니다. 이러한 ad-hoc teamwork는 다른 agent들을 fixed되었다고 보고 문제를 해결하는 단점이 있습니다.

Thomas는 common knowledge와 협력의 심리학적인 부분에 대해 연구하였습니다. Rubinstein은 common knowledge를 추정하기 위한 방법에 대해 보였습니다. Korkmaz는 common knowledge를 페이스북같은 communicaiton을 할 때에 대해 실험했습니다. Brafman은 협력을 개선하기 위해 common knowledge protocol을 사용하였습니다. 이는 MACKRL에서와 다르게 common knowledge를 common knowledge를 action set에 만들었습니다.

 Aumman은 correlated equilibrium의 개념에 대해 소개했습니다. 여기서는 협력을 잘하도록 돕는 공유되는 correlation device가 존재합니다. Cigler는 공유되는 coorelation vector와 communication channel이 있을 때, 어떻게 agent가 평형상태에 이를 수 있는지에 대해 연구했습니다. Boutilier는  fully observable환경에서  확실히 협력을 하기 위해 state space를 확장하는데, MACKRL은 shared commnication channel이나 full observability가 아니어도 common knowledge를 통해 이를 해결합니다.

Amato는 MacDec-POMDPs를 제안하고, Liu는 이런 모델을 환경에서 어떻게 배우는지에 대해 조사합니다. Makar는 MAXQ single-agent hierarchical framework을 multi-agent domain으로 확장하고 policies가 joint-action value function을 배우도록 합니다. MACKRL과 다르게 이는 execution할 때 communication이 필요합니다.

Kumar는 각 agent가 해결할 subtask를 만들고, action을 선택하기 위해 communicate할 agent쌍을 선택하는 hierarchical controller를 사용합니다. 이는 MACKRL과 다르게 실제 sequential decision making task에 test되지 않았습니다.

