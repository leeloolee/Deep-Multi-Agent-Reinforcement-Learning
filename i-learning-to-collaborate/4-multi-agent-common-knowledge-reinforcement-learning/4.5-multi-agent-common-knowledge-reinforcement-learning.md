# 4.5 Multi-Agent Common Knowledge Reinforcement Learning

MACKRL의 주요 포인트는 협력하는 decentralised agent라는 점입니다. MACKRL은  common knowledge를 공유하는 agent들 끼리의 joint policy $$ \pi^\mathcal{G}(\bold{u}^\mathcal{G}_{env}|\mathcal{L}^\mathcal{G}(\tau^a))$$를 만드는데 이는 centralised되었지만, decentralised한 방법으로 실행됩니다.\(뒤에서 좀더 자세히 설명할 것입니다.\) 모든 agent는 common knowledge와 같은 random seed를 통해 그들이 속한 그룹의 joint action에서 action을 sampling하는 형식으로 이루어 집니다. 이때 common knowledge가 충분한 정보를 가지고 있으면, 그룹의 policy는 꽤 좋은 joint action을 내놓을 것입니다. 하지만, 충분하지않다면 작은 subgroup으로 분할됩니다. subgroup간에는 더이상 협력이 일어나진 않지만,\(joint action select에 서로 영향을 미치지않지만\) 더 풍부한 common knowledge를 사용할 수 있을 것입니다. 이 모든 과정은 partially observable 했던 trajectories를 통한 common knowledge $$ \mathcal{L}^\mathcal{G}(\tau^a)$$에 의해서만 일어나므로 decentralised되었다고 볼 수있습니다. 이러한 아이디어를 실현하기 위해 hierarchy controller를  사용하였는데, 맨 상단과 중간 level의 controller에서는 joint action을 select하거나, subgroup으로 나누는 역할을 하고, 맨마지막에선 joint action에서 action을 select하는 행위를 합니다

