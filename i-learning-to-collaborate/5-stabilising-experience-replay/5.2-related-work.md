# 5.2 Related Work

value-based method의 빠른 학습에 대한 이전 연구들을 살펴보자면, DQN에서 Experience Memory의 data마다 TD error에 따른 priority를 붙여주어 성능을 높인 사례가 있었는데, 이는 MARL상황에서의 적용이 어렵습니다. 

hyper Q-learning이나 AWESOME의 idea는 environment의 non-stationarity를 현재 팀내 agent policy에 따른 agent의 학습과정을 tracking하거나 conditioning을 통해 해결하려했습니다. 하지만 이러한 해결 방법들은 high-dimension에 적용된 성공적인 사례를 보여주지 못했습니다.

Lauer는 distributed Q-learning을 변형시킨 방법을 제시했으나, 단순히 value function을 추정하는 일반적인 model-free방법으로는 MARL을 해결하기 어렵다고 말했습니다.

Wang은 off-policy experience를 single actor-critic method에 적용했으나, 이는 큰 variance를가질 때, truncate하는 방식이라, 결국 bias가 생깁니다. 하지만 여기서는 off-environment learning을 truncate없이 해결합니다.



