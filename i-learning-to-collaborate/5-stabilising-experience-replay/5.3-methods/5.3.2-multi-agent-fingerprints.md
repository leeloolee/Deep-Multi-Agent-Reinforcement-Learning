# 5.3.2 Multi-Agent Fingerprints

Importance Sampling을 통해서는 unbiased estimate을 얻을 순 있지만, importance weights자체가 unbound되어있기 때문에, variance를 제어할 방법이 또 필요합니다. 보통 truncate해서 사용할 수 있지만, 이를 사용하는 것 자체가 실험을 통해 적당한 값을 찾아야만하고, 이는 truncate하는 양이 클수록 biased됩니다. 결과적으로 이 chapter에서는 MARL에서 발생하는 non-stationary를 교정하기보다 포용하는 방식으로 진행됩니다.

다시 한번 IQL의 단점을 살펴보자면 다른 agents의 policies가 변하기 때문에 non-stationary하다는 점인데, 그렇다면 이는 다른말로, 다른 agent들의 policies를 조건에 두어 Q function을 만들면 stationary해진다는 것 입니다. 이 아이디어는 hyper Q-learning에서의 아이디어를 기반으로 합니다. 각 agent들의 state space는 다른 agents들의 policies를 Bayesian inference를 통해 추정한 값을 가집니다. 이는 직관적으로 각각의 agent의 학습을 stationary하게하지만 환경이 복잡해짐을 의미합니다.

현실적으로 이 hyper Q-learning의 단점은, Q function의 차원이 커질수록 학습이 어려워진다는 점입니다. hyper Q-learning의 network가 다른 agent의 policies를 observation function에 넣기위해 다른 모든 agent들의 network parameter $$ \theta^{-a}$$를 넣는다면, 새로운 observation function은 다음과 같이 정의됩니다. $$ O'(s) = \{O(s),\theta^{-a}\} $$당연하게도 이를 Neural Network에 이용하는 것은 $$ \theta^{-a}$$의 크기를 감당할 수 없습니다.

하지만 생각해보면, replay memory에서 우리가 사용하는 한 agent의 action probability는 policy network의 큰 parameter space에서 생성된 scalar입니다. 만약 각 agent의 observation이 이 trajectories가 어디서왔는지 충분히 명확하다면 이는 experience replay를 안정적으로 만들 수 있습니다.

그렇다면, 어떻게 충분한 정보를 가진 낮은 차원의 fingerprint\(색인 정도로 해석할 수 있겠지만\)를 만들 수 있을 것인지에 대한 대답을 구하면 됩니다. 이는 실제 다른 agent들의 state-action pair와 correlated되어야하며, policy학습에 generalization을 할 수 있어야 합니다. fingerprint의 한 후보로는 training iteration number가 있는데 이 문제점은 policies가 수렴한다면 여러 fingerprint에 대해 같은 값을 가져 결국 학습에 문제를 줄 수 있습니다.

다른 후보로는 exploration rate입니다. 이는 계속 감쇄하는 성질을 가지고 있는데, 이는 꽤나 괜찮은 성질을 가지고 있습니다. 학습이 진행될수록 낮은 값을 가지고 있고, 학습의 성능에도 크게 correlated됐기 때문입니다. 그러므로 Q-function에 이 exploration rate $$ \epsilon$$를 넣어 학습시키는 것이 괜찮은 대안이 될 수 있다는 것을 생각해보았고 이후에 실험에서 이게 꽤나 효과적임을 볼 수 있습니다.

