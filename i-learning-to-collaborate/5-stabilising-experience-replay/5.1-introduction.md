# 5.1 Introduction

Chapter 3,4를 통해 on-policy MARL에 대해 배워보았습니다. 하지만 RL자체에서 해결해야하는 필연적인 문제중 하나는 Sample efficiency입니다. 이 때, on-policy는 필연적으로 같은 데이터에 대해 여러번 학습하거나, 여러가지 policy로 부터 배울 수 있는 off-policy보다 Sample-efficiency가 낮을 수 밖에 없고 이는 MARL에서의 off-policy를 활용하는 방면으로 연구를 일으킬 수밖에 없었습니다. 

off-policy의 대표적인 algorithm인 DQN의 MARL에 대한 적용은 IQL입니다. 이 때, 환경에 존재하는 다른 agent들을 모두 정적인 존재로 취급해 해결을 하기 때문에, 수렴을 보장할 수가 없습니다. 하지만 다행히도 실전적으로 몇개의 실험에 대해서는 IQL이 꽤 괜찮은 성능을 보임을 보았습니다.

RL에서의 큰 발전을 이끌었던 요소중 하나에 Replay memory를 빼놓을 수 없는데, 이는 data를 iid로 만들어 Neural Network의 학습안정성에 도움을 줄 뿐만 아니라 efficiency도 높여줍니다. 하지만 이러한 Replay Memory도 IQL에 적용하기에는 문제점이 있습니다. MARL상황에서의 Replay Memory내의 data들은 현재환경의 dynamic을 표현하기 어렵습니다. 다른 agent들에 의해 non-stationary하기 때문입니다. 그러나 IQL을 보면 점진적으로 어떻게든 배우는 경향은 있으나, non-stationary한 data를 계속 sampling해 그냥 업데이트하는 행위는 결과적으로 agent의 학습에 큰 장애물이 아닐 수 없습니다.

이러한 문제점을 해결하기 위해 이전엔 Chapter 6에서 보겠지만, Replay Memory 크기를 작게 유지해 최근의 데이터만 사용하는등 sample efficiency를 낮추고, 근본적으로 MARL의 stability를 유지하며 학습할 수 있는 방법에 대해서는 설명하지 못했습니다. 결국 IQL에서의 Replay Memory 적용을 어떻게 시킬지가 또 해결해야할 어려운 문제로 남게되었습니다.

이 chapter에서는 Replay Memory를 MARL에 적용할 수 있는 두가지 효과적인 방법을 제시합니다.

* 첫째로, Replay Memory내의 data를 off-environment data로 취급하는 것입니다. off policy에서는 policy에 의해 등장하는 state distribution의 차이 때문에 Importance Sampling을 사용했다면, 이번에는 agent 입장에서의 다른 agent들의 joint action에 대해 distribution이 달라져 그에 대한 Importance Sampling을 진행합니다. 
* 둘째로, Hyper Q-learning에 의해 영감을 받은 접근법을 소개하는데, 이는 각 agent가 다른 agent의 policy들을 관찰하며 추정하여 non-stationary를 피합니다. 반면에 Q-function의 space가 커질 때 이를 감당할 수 없는데, 여기서는 작은 차원의 fingerprint를 이전의 한계를 해결합니다.

그리고 이러한 방법들에 대해 Starcraft unit micromanagement 환경에서 성공적인 성능을 보여줍니다.

