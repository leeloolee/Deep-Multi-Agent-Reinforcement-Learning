# 3.1 Introduction

MARL의 가장 큰 challenge중에 하나는 Credit Assignment Problem입니다. cooperative 상황에서, 환경은 주로 전체를 합산한 scalar 값의 global reward를 주기 때문에 이를 어느 agent의 기여로 인한 것인지 추론하기 위해선 단일 agent상황에서보다 많은 고려를 해주어야 합니다. 그렇기 때문에 이전엔 이를 agent 각자 reward를 받도록 reward 자체를 디자인한 사례들이 있는데, 이는 더 좋은 결과를 위해 어느 agent가 팀을 위해 희생을 해야하는 등의 상황에서의 학습을 어렵게 합니다.

이 챕터에서는 Counterfactual Multi-Agent Policy Gradient\(COMA\) policy gradient를 보입니다. COMA는 다음과 같은 세 가지 메인 아이디어로 구성이 되어있습니다.

* 첫 째로, COMA는 centralized critic을 이용합니다. 그리고 agent\(policy\)는 decentralized되어 있습니다. critic은 centralized되어 joint action에 대해 Credit Assignment Problem을 완화할 수 있고, observation이 아닌 state를 이용하기 때문에, 개별 agent보다 정확한 value를 계산할 수 있습니다.
* 둘 째로, counterfactual baseline을 이용합니다. 이 아이디어는 difference rewards라는 아이디어에서 영감을 받았는데, 이는 각자의 agent가 global reward에서 현재 action이 아닌 default action을 취했을 때의 받을 reward를 비교해서\(빼서\) 만듭니다. 이는 가능만하다면 Credit Assignment Problem을 해결할 수 있는 좋은 방법입니다. 하지만 이 default action을 정하는게 문제가되는데, 이를 COMA는 centralized critic이 다른 agent들의 action을 고정시킨 뒤 marginal action에 대한 advantage function을 구하는 방법으로 해결합니다. 즉 COMA에서는 각자의 agent마다 action에 대한 baseline을 만듭니다.
* 셋 째로, COMA에서는 효과적으로 계산하기 위해, 이를 단 한번의 network forward pass로 모든 agent에 대한 Q value를 계산하도록 만들었습니다.

COMA는 Starcraft unit micromanagement라는 최근에 부상하고있는 RL 벤치마크를 이용해 실험을 진행하였습니다. 이미 이전의 연구결과들에서 centralized control policy를 이용해 해결을 한 사례가 있으나, 여기서는 의미있는 decentralized policy에 대한 벤치마크를 보이기 위해 agent의 시야를 좁히고 이전에 사용했던 편리한 action들을 줄이면서 진행해보았습니다. 

이 때, COMA는 다른 multi-agent method보다 좋은 성능을 보였으며, 심지어 COMA의 best policy는 이전의 full state information을 받는 centralized policy와도 견주어 볼만했습니다.

