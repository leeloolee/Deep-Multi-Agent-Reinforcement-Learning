# 3.2 Related Work

이전 MARL연구들을 보자면, 처음엔 굉장히 간단한 환경에 대한 실험으로 시작되었고, 이때 앞에서 보았던 IQL의 등장과 two player pong으로의 적용 이후 DMARL에 대한 큰 기틀이 되었습니다.

또한 agent간의 communication에 대한 필요성을 느끼고, 이에 대한 연구도 이루어졌는데, 이는 하나는 agent간의 gradient를 흘려보내는 방식과 parameter를 sharing하는 방식 두가지가 주요한 방식으로 연구되었습니다. 하지만 이러한 방식들이 learning중 추가적인 state information\(centralised critic이 global state을\)을 사용하지 않았고, Credit Assignment Problem을 해결하지 않았다는 점에서 한계가 있습니다.

또한 Gupta, Egorov, Kochenderfer의 연구에서 centralised training, decentralised execution을 적용한 actor-critic연구가 진행되었으나, agent모두 local observation critic을 가지고, credit assignment problem를 오직 local reward를 만들어서 한점에서 한계가 있다고 볼 수 있습니다.

RL의 starcraft micromanagement 적용은 주로 multi agent에 대한 architecture특성은 사용하면서도 centralised controller와 full state를 사용하는 연구들이 진행되었습니다. [Usuiner의 연구](https://arxiv.org/pdf/1609.02993.pdf)에서는 greedy MDP를 사용했는데 이는 각 timestep에서 다른 agent들의 이전의 action들이 모두 주어진상태에서  action을 선택하는 방식으로 이루어집니다. 이는 논문의 Zero-order \(ZO\) backpropagation algorithm을 보면 이해할 수 있습니다. [Peng의 연구](https://arxiv.org/abs/1703.10069)에서는 RNN을 통해 agent간의 정보 교류가 일어나도록 설계하였습니다. 이때 Usunier의 연구에서 여기서 쓰인 비슷한 실험정의를 하였으며, DQN baseline을 만들었습니다. Omidshafiei의 연구에서는 decentralised training중의 experience replay 안정성을 해결하였습니다.

Rashid와 Sunehag의 연구에서는 agent 각자의 centralised critic을 제안했고, [Lowe의 연구](https://arxiv.org/pdf/1706.02275.pdf)에서는  centralised critic\(여기서는 single이라고 써져있는데 MADDPG자체가 여러개의 q network를 가지고 있어서 어떤 의미에서 single을 말하는지 의아했음\)을 제안하고 이를 decentralised actor를 학습하는데 사용했다. 이는 COMA와 유사한 면을 가지고 있는데, 실제로 이 연구는 여기서 제시하는 아이디어와 거의 동시에 이루어졌다. 하지만 여기서는 Credit Assignment Problem을 해결할 어떤 접근도 하지 않았다.



