# 3.4.2 Counterfactual Multi-Agent Policy Gradients

이번 section에서는 COMA에 대해 배우는데, 이는 앞에서 살펴보았듯 세 가지 주요 아이디어가 있습니다. 첫째로, centralized critic을 이용하는 것과, 둘 째로 counterfactual baseline을 사용하는 것, 셋 째로, baseline을 효과적으로 얻기위한 critic의 representation을 사용하는 것입니다.

![](../../../.gitbook/assets/marl_2%20%281%29.png)

* 첫번째 centralized critic에 대한 설명을 하겠습니다.  
  * IAC에서는 각자의 agent가 policy와 critic을 모두 가지고 있는데 이를 $$ \pi(u^a|\tau^a) $$와 $$ Q(\tau^a,u^a) $$혹은 $$V(\tau^a)$$로 나타낼 수 있니다. 이는 모두 agent의 history $$\tau$$에 의해 결정됩니다.
  * critic을 하나로, centralized시켜 가능하다면 global state로 학습시키는 아이디어는 그림\(a\)처럼 나타낼 수 있습니다. 
  * 가장 손쉽게 centralized critic을 update하는 식을 나타내면 다음과 같습니다.
  *                                      $$ g = \nabla_{\theta^\pi}\log{\pi(u|\tau^a_t)(r+\gamma V(s_{t+1})-V(s_t))}$$ 
  * 하지만 이 식은 Credit Assignment Problem을 해결하기에 적절하지 않습니다. 다른 agent들의 행동때문에 update자체가 굉장히 noisy해지기 때문에 올바른 방향으로 update되지 않을 수 있기 때문입니다.
*  그렇기 때문에, 두번째 특징인 counterfactual baseline을 사용해 이를 해결합니다. 이 아이디어는 difference rewards에서 영감을 받았는데, 이는 reward를 다음처럼 재설계합니다. 
  *                                                                 $$ D^a = r(s,\bold{u})-r(s,(\bold{u}^{-a},c^a))$$
  * 이는, joint action에 의한 global reward에서 다른 agent들의 action을 고정한채 agent a가 default action$$c^a$$를 취했을 때에 대한 reward를 빼면 현재 agent가 취한 action에 대한 정밀한 reward를 줄 수 있어 Credit Assignment Problem을 해결할 수 있다고 주장합니다. 이는 실제 agent가 action $$u^a$$를 취함으로써 얻는 real reward와 비슷해질 가능성이 더 높은데, 여기서 생기는 문제는 첫째로, 그럼 default action은 어떤 식으로 정할 것이냐가 문제가 됩니다. 둘째로는 두번의 global reward를 구하기 위한 추가적인 simulation이 필요해집니다. 
    * Tumer의 연구에서 이를 simulation으로 할게 아닌 function approximation을 통해 simulation을 덜해보자가 개선점이었는데 여전히 default action을 handcraft하게 정해줘야하는 문제점이 있었습니다.
    * COMA는 이러한 문제점 없이 difference rewards를 사용할 수 있는 방법을 사용했는데, 이는 COMA의 centralized critic이 $$ Q(s,\bold{u})$$를 학습하는데, 이를 agent 전부의 action에 대한 $$Q$$값을 single pass forward\(아래서 더 설명합니다.\)로 얻어내도록 architecturing해서 가능하도록 하였습니다. 수식은 다음과 같습니다.
    *                            $$ A^a(s,\bold{u}) = Q(s,\bold{u}) - \sum_{u'^{a}}{\pi^a(u'^{a}} |\tau)Q(s,(\bold{u}^{-a},u'^a))$$
    * $$ A^a(s,\bold{u})$$는 critic이 agent별로 marginal value를 주기 때문에, 추가적인 simulation이나 reward shaping등의 문제 없이 문제를 해결하게 되었습니다. 
* 마지막으로 이 모든 것을 single forward pass를 통해 해결합니다.
  * 만약 $$ \bold{u}$$에 대한 모든 Q를 한번에 계산해야한다 했을 때, critic의 output의 dimension은 $$ U^n$$이 될 것입니다. 이는 joint action space와 다름이 없고, 학습을 어렵게 만드는데, 이를 해결하기 위해 baseline을 이용하였습니다. 이 architecture는 그림 \(c\)를 통해 더 쉽게 이해할 수 있습니다.
  * 결과적으로 COMA의 output은 $$ U^n$$이 아닌 $$U$$가 됩니다.
* 여기서는 discrete한 상황에서의 예만 보이지만 continuous한 상황도 충분히 쉽게 확장할 수 있습니다.

알고리즘은 이 다음장에서 설명하도록 하겠습니다.

