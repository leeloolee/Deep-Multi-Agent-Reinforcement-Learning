# 3.4.1 Independent Actor-Critic

policy gradient를 MARL에 가장 간단하게 적용하는 방법은 IAC입니다. 이는 IQL를 actor critic으로 바꾼 형태인데, 이를 IAC라고 합니다. IAC는 학습 속도를 올리기 위해서 parameter sharing을 하는데, 이는 모든 agent가 다 같은 policy 와 critic을 가지게 됩니다. 그렇다고 모두 다 같은 행동을하는 것은 아닌데, 왜냐면, 같은 network를 쓰더라도 observation이 agent마다 다를 수 있기 때문입니다. 이때 critic은 joint action에 대한 가치 판단을 하는게 아닌 오직 한 agent에 대해서만 가치판단을 하고, IAC는 agent간의 정보 공유하는 법을 배우기 어렵고 이 때문에 협력적인 행동을 하는데 문제가 있습니다. 여기서는 다른 algorithm과의 성능 비교를 위한 baseline으로 쓰이게 됩니다.

이 때 IAC의 두가지 varient를 생각할 수 있는데, 첫째로, critic이 $$ V(\tau^a) $$를 estimate하는 경우와 $$Q(\tau^a,u^a) $$를 estimate하는 경우입니다. 둘다 그들의 개인적인 action이 정확하게 team에 얼마나 기여하는지 알기 어렵습니다.

