# 3.4.2.2 COMA Algorithm

[Konda의 연구](http://papers.neurips.cc/paper/1786-actor-critic-algorithms.pdf)에서는 actor critic이 local maximum에 수렴함을 보였는데, 다음의 조건들을 만족합니다.

1. policy $$\pi$$는 미분 가능 해야합니다.
2. $$Q$$와 $$\pi$$는 충분히 천천히 업데이트되어야하며 특히 $$\pi$$가 $$Q$$보다 더 천천히 업데이트 되어야 합니다.
3. $$Q$$와 $$\pi$$사이의 compatible한 조건이 있어야합니다.

![](../../../.gitbook/assets/marl_3%20%281%29.png)

### Architecturing & training

actor는 128개의 unit을 가진 GRU를 통해 만들어 졌고, GRU앞뒤로 fully connected layer가 연결되어 있습니다. IAC critic은 actor network last layer에 달린 last layer를 사용합니다. action은 마지막 layer에서 softmax를 통해 probability형식으로 나오게 됩니다. 여기에 lower bound $$\epsilon$$을 주었습니다. critic은 ReLU layer와 fully connected layer로 이루어져있으며, 여기서 나오는 hyperparameter들은 마린 5대5 시나리오에서 튜닝된뒤 다른맵에도 적용되었습니다. $$ \lambda $$는 0.8로 조정하였습니다.

### Ablations

여기선 3가지 키포인트들에 대해 실험하였습니다. 첫째로, critic의 centralized에 대해 IAC-Q와 IAC-V를 통해 비교했습니다. 둘째로, V보다 Q를 배우는 것에 대해 실험했습니다. central-V는 full state를 통해 학습하지만, V를 학습합니다. 그리고 TD erorr를 통해 advantage를 추정하여 agent를 update합니다. 셋째로, central-QV는 Q와V를 동시에 배우고 advantage를 바로 Q-V를 통해 얻습니다. 

