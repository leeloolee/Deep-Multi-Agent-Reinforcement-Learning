# 1.2 Deep Multi-Agent Reinforcement-Learning

 최근 몇년간 DRL의 진보는 정적인 task의 single agent환경에서 이루어 졌습니다. 반면에 현실의 많은 문제들은 multi-agent problem으로 나타납니다. 대부분의 상황에서 agent들은 전체적인 혹은 개인의 보상을 최대화하기 위해 다른 agent를 신경쓰면서 각각 불완전한 관찰을 통한 독립적인 결정을 내려야 합니다. 

 Multi-Agent RL은 그러한 문제를 해결할 수 있는 하나의 프레임워크입니다. MARL은 효과적인 policy를 찾을 수 있도록 learning rule을 발전시키고 분석하는 것에 관심을 둡니다.

 MARL은 다음과 같은 트랜드 때문에 중요한 역할을 할 것입니다.

* 첫째로, 머신러닝의 적용이 어디에서나 일어나고 있습니다. 이는 AI System이 주변의 AI system을 염두에 둬야 할 것임을 의미합니다. 
* 둘째로, 또 다른 이유는 MARL은 인간과 같은 시스템을 발전시키는데 있어 징검돌 역할을 할 수 있기 때문입니다. 사회적으로 많은 교류가 필요할 수록, 지능이 높아야합니다. 남의 생각을 말하고, 남의 상태를 말하는 것은 추상적 사고의 단계에 해당됩니다.

 여기선 agent에게 다음과 같은 능력을 학습시키는 것에 집중합니다.

* **collaboration**
  * 여러 agent의 action에 따라 한 agent의 입장에서 봤을 때 state distribution이 non-stationary해 집니다. 이는 agent의 학습을 어렵게 만듭니다. 
  * **credit assignment problem**
    * 이는 global reward가 scalar이기 때문에 발생합니다. 그렇기 때문에 각 agent가 어떤 행동때문에 좋은 보상 혹은 나쁜 보상을 얻었는지 할당하는 것 자체가 learning 과정에 포함되어 학습되어야 합니다. 또한 환경내의 다른 agent들은 각 agent에게 reward를 할당하는데 교란요인이 되어, 만약 한 agent가 최선의 선택을 하였다고 해도, 다른 agent의 행동과 joint되어 있어, policy를 배우는데 어려움이 생길 수 있습니다. 
* **communication**
  * Communication은 communication protocol을 배우는 문제와 연관되어 있습니다. 많은 상황에서 agent는 각자 따로 actions을 하도록 설계되고, 서로 제한된 communication channel을 가집니다. 어떻게 상황에 대한 information을 교환할지 학습하는것은 중요한 역할을 합니다.
* **reciprocity**
  * 이는 general-sum 환경에서 일어납니다. 상대의 policy에 따라 모두가 높은 reward를 받을 수도, 낮은 reward를 받을 수도 있습니다. 대표적으로 죄수의 딜레마 문제가 있습니다. 여기선 전체가 좋은 reward에 수렴할 수 있도록 하기 위해 어떤 policy를 가져야하는지 학습하는데 주력합니다.

