# 1.2 Deep Multi-Agent Reinforcement-Learning

 최근 몇년간 DRL의 진보는 정적인 task의 single agent 환경에서이루어 졌습니다. 반면에 현실의 많은 문제들은 multi-agent problem으로 나타납니다. 대부분의 상황에서 agent들은 전체적인 혹은 개인의 보상을 최대화하기 위해 다른 agent\(다른 learning algorithm을 가지고 있을 수 있습니다.\)를 신경쓰면서 각각 불완전한 관찰을 통한 독립적인 결정을 내려야 합니다. 

 Multi-Agent RL은 그러한 문제를 해결할 수 있는 하나의 프레임워크입니다. MARL은 효과적인 policy를 찾을 수 있도록 learning rule을 발전시키고 분석하는 것에 관심을 둡니다.

 MARL은 다음과 같은 트랜드 때문에 중요한 역할을 할 것입니다.

* 첫째로, 머신러닝의 적용이 어디에서나 일어나고 있습니다. 이는 AI System이 주변의 AI system을 염두에 둬야 할 것임을 의미합니다. 0
* 둘째로, 또 다른 이유는 MARL은 인간과 같은 시스템을 발전시키는데 있어 징검돌 역할을 할 수 있기 때문입니다. 사회적으로 많은 교류가 필요할 수록, 지능이 높아야합니다. 남의 생각을 말하고, 남의 상태를 말하기 위해선 더 추상적이어야 하기 때문입니다.

두번째 이유가 중요한 이유는, 미래의 지능을 가진 agent는 매일 사람과 교류하고 다른 agent들을 고려해야합니다. 사람은 이런 교류를 원활하게 하기 위해 추론하는 능력뿐만이 아니라 믿음이나 문화, 언어 등에 대한 많은 개념들을 포함하는데, 미래의 agent가 잘 소통하기 위해선 서로의 관점을 바꾸는 것도 중요하고, 이러한 개념들이 없이는 원활한 소통이 어려울 수 있습니다.

 그래서 여기선 위의 몇몇 문제를 해결할 수 있는 Multi-Agent RL Algorithm에 집중하는데, 여기선 agent의 collaboration, communication, reciprocity ability에 대한 내용을 담았습니다.

* Collaboration능력이 필요한 이유는 MARL definition에선 많은 수의 agent가 동시에 존재하며 state distribution이 nonstationary하게 계속 변하는 점에서 학습이 어려워지는 문제가 발생합니다. 이 안에는 agent간의 credit assignment problem도 포함됩니다. 이는 많은 agent가 함께 행동하지만, reward는 결국 scalar로 나오고, 어떤 행동때문에 좋은 보상 혹은 나쁜 보상을 얻었는지 할당하는 것 자체가 learning 과정에 포함되어 학습되어야 합니다. 또한 환경내의 다른 agent들은 agent에게 reward를 할당하는데 교란요인이 되어, 만약 한 agent가 최선의 선택을 하였다고 해도, 다른 agent의 행동과 joint되어 있어, policy를 배우는데 어려움이 생길 수 있습니다. 
* Communication은 communication protocol을 배우는 문제와 연관되어 있습니다. 많은 상황에서 agent는 각자 따로 actions을 하도록 설계되고, 서로 제한된 communication channel을 가집니다. 어떻게 상황에 대한 information을 교환할지 학습하는것은 어려운 문제를 푸는데 도움이 됩니다.
* 이렇듯 협력이 필요한 상황에서의 많은 문제는 해결되었지만, reciprocity는 다루기 어려운 문제입니다. 이러한 환경에서 agent는 다른 agent를 도왔을 때, 더많은 보상을 받습니다. 사람들은 이러한 능력이 자연스레 있지만, 이를 강화학습을 통해 학습시키는 것은 아직도 열린 문제입니다. 이러한 문제를 풀기 위해 여기서는 주로 학습할 때는 중앙에서 모든 정보를 준 채로 학습하고\(centralized\), 실행할땐 agent 개별의 local observation을 이용해 진행하는 것입니다.  이는 full observable한 critic이 개별의 value function보다 정확하기 때문에 효과적이라고 할 수 있습니다. 이는 심지어 non-cooperative 상황에서도 잘 작동합니다.

