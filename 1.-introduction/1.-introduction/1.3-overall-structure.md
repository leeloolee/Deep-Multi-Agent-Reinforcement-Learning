# 1.3  Overall Structure

이 후의 Section들은 Background에 대한 설명과 세 가지 파트로 나뉘는데, 여기서는 그 Section들에 대한 outline을 제시하고 간단한 overview를 진행합니다.

### Background

Background는 Chapter 2에서 다루게 됩니다. MARL에서 사용되는 formal한 setting과, 필요한 알고리즘, 개념적 지식을 설명합니다. 그 뒤의 내용들을 잘 이해하기 위해 Background의 내용은 확실히 소화하고 넘어가시길 바랍니다.

## Part 1 : Learning to Collaborate

Single agent의 테크닉을 그대로 MARL에 적용한 것을 Naive learning\(NL\) 이라고 합니다. 이 NL에서의 agent는 다른 상황을 모두 정적이라고 가정합니다. NL은 다른 agent에 대해 아무 가정없이 접근한다는 것이 키포인트입니다. NL은 신기하게도 생각보다 robust하고 좋은 성능을 보이는데, 그렇기에 다른 algorithm의 benchmark로 주로 사용됩니다.

이전에 언급했던 것 처럼 MARL에서의 큰 문제점은 credit assignment 문제인데, 한 agent가 최적의 행동을 하였어도 다른 agent때문에 전체의 reward가 줄어들면, 이러한 최적의 행동을 할 확률이 줄어들게 됩니다.이런 문제를 NL에서는 신경을 못쓰기 때문에 더욱 문제가 됩니다.

### Chapter 3 Counterfactual Multi-Agent Poilicy Gradidents\(COMA\)

Chapter 3에서는 COMA에 대해 배웁니다. COMA는 전체적인 state와 모든 agent의 joint action을 볼 수 있는 critic\(centralised critic\)을 가지고 학습을 진행합니다. 이때 baseline으로, 한 agent가 다른 action을 취했을 때의 평균에 대한 추정\(V'\)을 가지고 advantage function을 만듭니다. COMA는 starcraft 환경에서 baseline이 될만큼 괜찮은 성능을 보입니다.

COMA에서의 단점은 agent들의 action set $$ \bold{a}=<a_1,a_2,...,a_n>$$ 가 각각 policy들이 independent하므로, 확률을 나타낼 때 곱의 형태로 나타난다는 점입니다. 이는 결국, 어떠한 action set에 대한 학습이 set 내부의 action value들 서로간의 영향을 받는다는 의미입니다.

centralised critic이라고 하더라도 action의 선택에 대해서는 독립적이기 때문에 일어나는 일입니다. 그래서 아예 centralised policy를 두고 joint action에 대해 학습을 진행하는데, 이는 single agent를 action space를 combinatorial하게 정의한 것으로 볼 수 있어 효과적이진 않다고 생각합니다.

### Chapter 4 Multi-Agent Common Knowledge Reinforcement Learning\(MACKRL\)

Chapter 4 에서 여기서는 Multi-Agent Common Knowledge Reinforcement Learning\(MACKRL\)를 소개합니다. MACKRL 에서는 group내에서 agent들이 서로 common knowledge를 가지게 되는데, 이는 fully decentralised 상황에서 joint action을 배우기 위함입니다. 그룹의 common knowledge라 함은 모든 agent가 알고 모든 agent가 모든 agent가 안다는 것을 아는 상태를 말합니다. 흥미로운건 MARL의 여러 환경에서 agent는 다른 agent들을 관찰하고, 그럼으로써 그들은 common knowledge 를 가지게 됩니다. 특히 MACKRL은 hierarchical controller에 의존하는데, 이 것은 그룹의 agent들에게 joint action 을 할당해주거나, 어떻게 agent들을 어떻게 분할해 협동해야하는지를 정해줍니다. 중요한 것은 각각의 세부 subgroup들은 조직화된 action을 고르는데 있어선 부족하지만 common knowledge 를 공유할 것입니다. 그러므로 MACKRL controller는 엄청난 양의 정보를 가지고 있는 개개인의 actor와 subgroup의 조직화된 action사이에 trade off를 배울 것입니다.

COMA와 MACKRL은 on-policy method 이기 때문에, 현재의 policy만 가지고 학습을 하게 됩니다. 하지만, off-policy는 sample efficiency가 높습니다. 좀더 안정된 learning을 위해서, off-policy Deep RL은 replay memory를 사용하는데 초점이 맞추어져 있습니다.

그 multiple learning agent는 환경을 nonstationary하게 만듭니다. state-action 에 대한 reward는 다른 agent들에 독립적이지 않습니다. 모든 agent는 그들의 policy를 update하고, reward 기댓값은 계속해서 변하게 됩니다.

이러한 문제를 풀기위해서 안정화된 Replay buffer를 사용하는 새로운 방법을 소개하는데, centralised 된 학습중에 multi-agent에 importance weights 를 사용하고, metadata인 fingerprint를 넣어주는 방법입니다. fingerprint는 episode로 부터 분리시켜주고, 다른 agent들의 policy와 구분될 수 있게 해줍니다.

이 Chapter들은 다음의 논문에 따라 진행됩니다.

* 'Counterfactual Multi-Agent Policy Gradient' Jakob Foerster, et al.
* 'Stabilising Experience replay for deep multi-agent reinforcement learning' Jakob Foerster, et al.
* 'Multi-Agent Common-Knowledge Reinforcement Learning', Christian A Schroeder de Witt, et al.

### Part 2 : Learning to Communicate

지금까지는 agent간의 명확한 communication 방법에 대해 생각해보지 않았습니다. 하지만, 실제 적용단계에서는 communication방법이 굉장히 제한되어있습니다. Chapter 6 에서는 어떻게 agent가 Cheap-talk 채널만을 사용하면서 communication protocol을 배우는지 배울 것입니다. Cheap-talk이란 environment에서 transition probability나 reward에 영향을 미치지 않는, 즉, environment와 독립적인 channel이란 뜻입니다. 

Differentiable Inter-Agent Learning\(DIAL\)과 Reinforced Inter-Agent Learning\(RIAL\)을 제안하는데, centralised training동안, DIAL은 continuous activation으로써  messages를 모델링하고, 어떤 message를 보내야 할지에 대해 배우기 위해 gradients를 보냅니다.

하지만 RIAL은 메세지를 action space의 일부로 생각하고, 어떤 message를 보내야할지 배웁니다. Chapter 7에서는 Bayesian Action Decoder\(BAD\)를 제안하는데, 이는 RIAL의 확장된 버전입니다. DIAL와는 다르게, BAD는 cheap talk channel없이 사용될 수 있는데 특히, agent가 action을 취했을 때,  다른 agent로 부터 관찰됨으로써 communicate되도록 했습니다. 직접적으로 다른 agent를 관찰하며 그들의 상태를 추론함으로써, BAD는 Hanabi game에서 SOTA를 달성했습니다. 다음과 같은 논문들을 기반으로 쓰여졌습니다.

* 'Learning to communicate with deep multi-agent reinforcement learning' Jakob Foerster,et al.
* 'Bayesian Action Decoder for Deep Multi-Agent Reinforcement Learning' Jakob Foerster,et al.

### Part 3 : Learning to Reciprocate

지금까지는 cooperative한 환경에서 team reward를 최대화 하기 위한 방법들에 대해 배웠습니다. 하지만, 많은 실제 문제는 개인의 목적을 최대화하는 데에 목적을 두고 있다. 예를 들면, 모든 driver는 자신의 목적지까지 최대한 빨리 도착하는 것을 원하지 전체 도로의 효율성을 따지지 않는다. Game theory는 이러한 상황에서 최선의 전략을 짜는데 오랜 역사가 있다. 여기에는 내쉬 균형\(Nash equilibrium\)이라는 게임이론의 중점적인 개념이있는데, 어떤 agent도 자신이 policy를 바꾼다고 더 나은 return을 받지 못하는 균형상태를 말합니다. 하지만 게임이론에서 내쉬 균형은 계산을 통해 얻어질 수 있다고 알려지지만, RL에서는 다릅니다. MARL에서 agent는 environment와의 상호작용에 의존합니다. 반면에 NL‌  
이 후의 Section들은 Background에 대한 설명과 세 가지 파트로 나뉘는데, 여기서는 그 Section들에 대한 outline을 제시하고 간단한 overview를 진행합니다.  
  
‌  
Background  
‌  
Background는 Chapter 2에서 다루게 됩니다. MARL에서 사용되는 formal한 setting과, 필요한 알고리즘, 개념적 지식을 설명합니다. 그 뒤의 내용들을 잘 이해하기 위해 Background의 내용은 확실히 소화하고 넘어가시길 바랍니다.  
  
‌  
Part 1 : Learning to Collaborate  
‌  
Single agent의 테크닉을 그대로 MARL에 적용한 것을 Naive learning\(NL\) 이라고 합니다. 이 NL에서의 agent는 다른 상황을 모두 정적이라고 가정합니다. NL은 다른 agent에 대해 아무 가정없이 접근한다는 것이 키포인트입니다. NL은 신기하게도 생각보다 robust하고 좋은 성능을 보이는데, 그렇기에 다른 algorithm의 benchmark로 주로 사용됩니다.  
  
‌  
이전에 언급했던 것 처럼 MARL에서의 큰 문제점은 credit assignment 문제인데, 한 agent가 최적의 행동을 하였어도 다른 agent때문에 전체의 reward가 줄어들면, 이러한 최적의 행동을 할 확률이 줄어들게 됩니다.이런 문제를 NL에서는 신경을 못쓰기 때문에 더욱 문제가 됩니다.  
  
‌  
Chapter 3 Counterfactual Multi-Agent Poilicy Gradidents\(COMA\)  
‌  
Chapter 3에서는 Counterfactual Multi-Agent Poilicy Gradidents\(COMA\)에 대해 배울 건데, COMA는 centralised critic을 사용해서 모든 agent간의 행동에 대한 value function을 사용하기 때문에 이러한 한계점을 극복할 수 있다. 아래에 설명한 difference reward\(1\)에 착안해서 여기서는 value function을 counterfactual의 baseline을 계산하는데 사용했는데, 이 baseline은 다른 agent가 어떤 행동을 했을 때 어떤 일이 일어날지에 대한 평균에 대한 추정입니다. 이를 StarCraft micromanagement에 적용했는데, 좋은 baseline이 되었습니다.  
  
‌  
COMA는 agent들의 행동을 완전히 요소화하여 \(2\)joint value function을 배우게 됩니다. 다른말로, action이 여러 다른 agent들의 의한 joint action이 되도록 독립적으로 sampling 한다는 말입니다. 하지만 몇몇의 상황에서는 이러한 이러한 구성이 최적의 전략을 짜지 못할 수 있습니다.   
  
‌  
특히 한 agent의 최적의 행동이 다른 agent의 최적의 action을 뽑는데 독립적이지 않다면,  최적의 행동과 벗어난 행동을 하도록 만들 수 있습니다.  
  
‌  
이러한 문제는 centralised controller에 의해 해결될 수 있는데, decentralised 된 execution에는 잘 작동하지 않을 수 있습니다.  
  
‌  
Chapter 4 Multi-Agent Common Knowledge Reinforcement Learning\(MACKRL\)  
‌  
Chapter 4 에서 여기서는 Multi-Agent Common Knowledge Reinforcement Learning\(MACKRL\)를 소개합니다. MACKRL 에서는 group내에서 agent들이 서로 common knowledge를 가지게 되는데, 이는 fully decentralised 상황에서 joint action을 배우기 위함입니다. 그룹의 common knowledge라 함은 모든 agent가 알고 모든 agent가 모든 agent가 안다는 것을 아는 상태를 말합니다. 흥미로운건 MARL의 여러 환경에서 agent는 다른 agent들을 관찰하고, 그럼으로써 그들은 common knowledge 를 가지게 됩니다. 특히 MACKRL은 hierarchical controller에 의존하는데, 이 것은 그룹의 agent들에게 joint action 을 할당해주거나, 어떻게 agent들을 어떻게 분할해 협동해야하는지를 정해줍니다. 중요한 것은 각각의 세부 subgroup들은 조직화된 action을 고르는데 있어선 부족하지만 common knowledge 를 공유할 것입니다. 그러므로 MACKRL controller는 엄청난 양의 정보를 가지고 있는 개개인의 actor와 subgroup의 조직화된 action사이에 trade off를 배울 것입니다.  
  
‌  
COMA와 MACKRL은 on-policy method 이기 때문에, 현재의 policy만 가지고 학습을 하게 됩니다. 하지만, off-policy는 sample efficiency가 높습니다. 좀더 안정된 learning을 위해서, off-policy Deep RL은 replay memory를 사용하는데 초점이 맞추어져 있습니다.  
  
‌  
그 multiple learning agent는 환경을 nonstationary하게 만듭니다. state-action 에 대한 reward는 다른 agent들에 독립적이지 않습니다. 모든 agent는 그들의 policy를 update하고, reward 기댓값은 계속해서 변하게 됩니다.  
  
‌  
이러한 문제를 풀기위해서 안정화된 Replay buffer를 사용하는 새로운 방법을 소개하는데, centralised 된 학습중에 multi-agent에 importance weights 를 사용하고, metadata인 fingerprint를 넣어주는 방법입니다. fingerprint는 episode로 부터 분리시켜주고, 다른 agent들의 policy와 구분될 수 있게 해줍니다.  
  
‌  
이 Chapter들은 다음의 논문에 따라 진행됩니다.  
  
‌  
'Counterfactual Multi-Agent Policy Gradient' Jakob Foerster, et al.  
  
'Stabilising Experience replay for deep multi-agent reinforcement learning' Jakob Foerster, et al.  
  
'Multi-Agent Common-Knowledge Reinforcement Learning', Christian A Schroeder de Witt, et al.  
  
‌  
Part 2 : Learning to Communicate  
‌  
지금까지는 agent간의 명확한 communication 방법에 대해 생각해보지 않았습니다. 하지만, 실제 적용단계에서는 communication방법이 굉장히 제한되어있습니다. Chapter 6 에서는 어떻게 agent가 Cheap-talk 채널만을 사용하면서 communication protocol을 배우는지 배울 것입니다. Cheap-talk이란 environment에서 transition probability나 reward에 영향을 미치지 않는, 즉, environment와 독립적인 channel이란 뜻입니다.   
  
‌  
Differentiable Inter-Agent Learning\(DIAL\)과 Reinforced Inter-Agent Learning\(RIAL\)을 제안하는데, centralised training동안, DIAL은 continuous activation으로써  messages를 모델링하고, 어떤 message를 보내야 할지에 대해 배우기 위해 gradients를 보냅니다.  
  
‌  
하지만 RIAL은 메세지를 action space의 일부로 생각하고, 어떤 message를 보내야할지 배웁니다. Chapter 7에서는 Bayesian Action Decoder\(BAD\)를 제안하는데, 이는 RIAL의 확장된 버전입니다. DIAL와는 다르게, BAD는 cheap talk channel없이 사용될 수 있는데 특히, agent가 action을 취했을 때,  다른 agent로 부터 관찰됨으로써 communicate되도록 했습니다. 직접적으로 다른 agent를 관찰하며 그들의 상태를 추론함으로써, BAD는 Hanabi game에서 SOTA를 달성했습니다. 다음과 같은 논문들을 기반으로 쓰여졌습니다.  
  
‌  
'Learning to communicate with deep multi-agent reinforcement learning' Jakob Foerster,et al.  
  
'Bayesian Action Decoder for Deep Multi-Agent Reinforcement Learning' Jakob Foerster,et al.  
  
‌  
Part 3 : Learning to Reciprocate  
‌  
지금까지는 cooperative한 환경에서 team reward를 최대화 하기 위한 방법들에 대해 배웠습니다. 하지만, 많은 실제 문제는 개인의 목적을 최대화하는 데에 목적을 두고 있다. 예를 들면, 모든 driver는 자신의 목적지까지 최대한 빨리 도착하는 것을 원하지 전체 도로의 효율성을 따지지 않는다. Game theory는 이러한 상황에서 최선의 전략을 짜는데 오랜 역사가 있다. 여기에는 내쉬 균형\(Nash equilibrium\)이라는 게임이론의 중점적인 개념이있는데, 어떤 agent도 자신이 policy를 바꾼다고 더 나은 return을 받지 못하는 균형상태를 말합니다. 하지만 게임이론에서 내쉬 균형은 계산을 통해 얻어질 수 있다고 알려지지만, RL에서는 다릅니다. MARL에서 agent는 environment와의 상호작용에 의존합니다. 반면에 NL은 cooperative한 환경에선 놀랄만큼 잘 동작했지만, general sum issue가 발 생할 수 있다.. 첫번째로, 모든 agent는 그들의 objective를 최대화하는 것은 안정적이지 않고, 둘째로, agent는 상ion들은 Background에 대한 설명과 세 가지 파트로 나뉘는데, 여기서는 그 Section들에 대한 outline을 제시하고 간단한 overview를 진행합니다.  
  
‌  
Background  
‌  
Background는 Chapter 2에서 다루게 됩니다. MARL에서 사용되는 formal한 setting과, 필요한 알고리즘, 개념적 지식을 설명합니다. 그 뒤의 내용들을 잘 이해하기 위해 Background의 내용은 확실히 소화하고 넘어가시길 바랍니다.  
  
‌  
Part 1 : Learning to Collaborate  
‌  
Single agent의 테크닉을 그대로 MARL에 적용한 것을 Naive learning\(NL\) 이라고 합니다. 이 NL에서의 agent는 다른 상황을 모두 정적이라고 가정합니다. NL은 다른 agent에 대해 아무 가정없이 접근한다는 것이 키포인트입니다. NL은 신기하게도 생각보다 robust하고 좋은 성능을 보이는데, 그렇기에 다른 algorithm의 benchmark로 주로 사용됩니다.  
  
‌  
이전에 언급했던 것 처럼 MARL에서의 큰 문제점은 credit assignment 문제인데, 한 agent가 최적의 행동을 하였어도 다른 agent때문에 전체의 reward가 줄어들면, 이러한 최적의 행동을 할 확률이 줄어들게 됩니다.이런 문제를 NL에서는 신경을 못쓰기 때문에 더욱 문제가 됩니다.  
  
‌  
Chapter 3 Counterfactual Multi-Agent Poilicy Gradidents\(COMA\)  
‌  
Chapter 3에서는 Counterfactual Multi-Agent Poilicy Gradidents\(COMA\)에 대해 배울 건데, COMA는 centralised critic을 사용해서 모든 agent간의 행동에 대한 value function을 사용하기 때문에 이러한 한계점을 극복할 수 있다. 아래에 설명한 difference reward\(1\)에 착안해서 여기서는 value function을 counterfactual의 baseline을 계산하는데 사용했는데, 이 baseline은 다른 agent가 어떤 행동을 했을 때 어떤 일이 일어날지에 대한 평균에 대한 추정입니다. 이를 StarCraft micromanagement에 적용했는데, 좋은 baseline이 되었습니다.  
  
‌  
COMA는 agent들의 행동을 완전히 요소화하여 \(2\)joint value function을 배우게 됩니다. 다른말로, action이 여러 다른 agent들의 의한 joint action이 되도록 독립적으로 sampling 한다는 말입니다. 하지만 몇몇의 상황에서는 이러한 이러한 구성이 최적의 전략을 짜지 못할 수 있습니다.   
  
‌  
특히 한 agent의 최적의 행동이 다른 agent의 최적의 action을 뽑는데 독립적이지 않다면,  최적의 행동과 벗어난 행동을 하도록 만들 수 있습니다.  
  
‌  
이러한 문제는 centralised controller에 의해 해결될 수 있는데, decentralised 된 execution에는 잘 작동하지 않을 수 있습니다.  
  
‌  
Chapter 4 Multi-Agent Common Knowledge Reinforcement Learning\(MACKRL\)  
‌  
Chapter 4 에서 여기서는 Multi-Agent Common Knowledge Reinforcement Learning\(MACKRL\)를 소개합니다. MACKRL 에서는 group내에서 agent들이 서로 common knowledge를 가지게 되는데, 이는 fully decentralised 상황에서 joint action을 배우기 위함입니다. 그룹의 common knowledge라 함은 모든 agent가 알고 모든 agent가 모든 agent가 안다는 것을 아는 상태를 말합니다. 흥미로운건 MARL의 여러 환경에서 agent는 다른 agent들을 관찰하고, 그럼으로써 그들은 common knowledge 를 가지게 됩니다. 특히 MACKRL은 hierarchical controller에 의존하는데, 이 것은 그룹의 agent들에게 joint action 을 할당해주거나, 어떻게 agent들을 어떻게 분할해 협동해야하는지를 정해줍니다. 중요한 것은 각각의 세부 subgroup들은 조직화된 action을 고르는데 있어선 부족하지만 common knowledge 를 공유할 것입니다. 그러므로 MACKRL controller는 엄청난 양의 정보를 가지고 있는 개개인의 actor와 subgroup의 조직화된 action사이에 trade off를 배울 것입니다.  
  
‌  
COMA와 MACKRL은 on-policy method 이기 때문에, 현재의 policy만 가지고 학습을 하게 됩니다. 하지만, off-policy는 sample efficiency가 높습니다. 좀더 안정된 learning을 위해서, off-policy Deep RL은 replay memory를 사용하는데 초점이 맞추어져 있습니다.  
  
‌  
그 multiple learning agent는 환경을 nonstationary하게 만듭니다. state-action 에 대한 reward는 다른 agent들에 독립적이지 않습니다. 모든 agent는 그들의 policy를 update하고, reward 기댓값은 계속해서 변하게 됩니다.  
  
‌  
이러한 문제를 풀기위해서 안정화된 Replay buffer를 사용하는 새로운 방법을 소개하는데, centralised 된 학습중에 multi-agent에 importance weights 를 사용하고, metadata인 fingerprint를 넣어주는 방법입니다. fingerprint는 episode로 부터 분리시켜주고, 다른 agent들의 policy와 구분될 수 있게 해줍니다.  
  
‌  
이 Chapter들은 다음의 논문에 따라 진행됩니다.  
  
‌  
'Counterfactual Multi-Agent Policy Gradient' Jakob Foerster, et al.  
  
'Stabilising Experience replay for deep multi-agent reinforcement learning' Jakob Foerster, et al.  
  
'Multi-Agent Common-Knowledge Reinforcement Learning', Christian A Schroeder de Witt, et al.  
  
‌  
Part 2 : Learning to Communicate  
‌  
지금까지는 agent간의 명확한 communication 방법에 대해 생각해보지 않았습니다. 하지만, 실제 적용단계에서는 communication방법이 굉장히 제한되어있습니다. Chapter 6 에서는 어떻게 agent가 Cheap-talk 채널만을 사용하면서 communication protocol을 배우는지 배울 것입니다. Cheap-talk이란 environment에서 transition probability나 reward에 영향을 미치지 않는, 즉, environment와 독립적인 channel이란 뜻입니다.   
  
‌  
Differentiable Inter-Agent Learning\(DIAL\)과 Reinforced Inter-Agent Learning\(RIAL\)을 제안하는데, centralised training동안, DIAL은 continuous activation으로써  messages를 모델링하고, 어떤 message를 보내야 할지에 대해 배우기 위해 gradients를 보냅니다.  
  
‌  
하지만 RIAL은 메세지를 action space의 일부로 생각하고, 어떤 message를 보내야할지 배웁니다. Chapter 7에서는 Bayesian Action Decoder\(BAD\)를 제안하는데, 이는 RIAL의 확장된 버전입니다. DIAL와는 다르게, BAD는 cheap talk channel없이 사용될 수 있는데 특히, agent가 action을 취했을 때,  다른 agent로 부터 관찰됨으로써 communicate되도록 했습니다. 직접적으로 다른 agent를 관찰하며 그들의 상태를 추론함으로써, BAD는 Hanabi game에서 SOTA를 달성했습니다. 다음과 같은 논문들을 기반으로 쓰여졌습니다.  
  
‌  
'Learning to communicate with deep multi-agent reinforcement learning' Jakob Foerster,et al.  
  
'Bayesian Action Decoder for Deep Multi-Agent Reinforcement Learning' Jakob Foerster,et al.  
  
‌  
Part 3 : Learning to Reciprocate  
‌  
지금까지는 cooperative한 환경에서 team reward를 최대화 하기 위한 방법들에 대해 배웠습니다. 하지만, 많은 실제 문제는 개인의 목적을 최대화하는 데에 목적을 두고 있다. 예를 들면, 모든 driver는 자신의 목적지까지 최대한 빨리 도착하는 것을 원하지 전체 도로의 효율성을 따지지 않는다. Game theory는 이러한 상황에서 최선의 전략을 짜는데 오랜 역사가 있다. 여기에는 내쉬 균형\(Nash equilibrium\)이라는 게임이론의 중점적인 개념이있는데, 어떤 agent도 자신이 policy를 바꾼다고 더 나은 return을 받지 못하는 균형상태를 말합니다. 하지만 게임이론에서 내쉬 균형은 계산을 통해 얻어질 수 있다고 알려지지만, RL에서는 다릅니다. MARL에서 agent는 environment와의 상호작용에 의존합니다. 반면에 NL‌  
이 후의 Section들은 Background에 대한 설명과 세 가지 파트로 나뉘는데, 여기서는 그 Section들에 대한 outline을 제시하고 간단한 overview를 진행합니다.  
  
‌  
Background  
‌  
Background는 Chapter 2에서 다루게 됩니다. MARL에서 사용되는 formal한 setting과, 필요한 알고리즘, 개념적 지식을 설명합니다. 그 뒤의 내용들을 잘 이해하기 위해 Background의 내용은 확실히 소화하고 넘어가시길 바랍니다.  
  
‌  
Part 1 : Learning to Collaborate  
‌  
Single agent의 테크닉을 그대로 MARL에 적용한 것을 Naive learning\(NL\) 이라고 합니다. 이 NL에서의 agent는 다른 상황을 모두 정적이라고 가정합니다. NL은 다른 agent에 대해 아무 가정없이 접근한다는 것이 키포인트입니다. NL은 신기하게도 생각보다 robust하고 좋은 성능을 보이는데, 그렇기에 다른 algorithm의 benchmark로 주로 사용됩니다.  
  
‌  
이전에 언급했던 것 처럼 MARL에서의 큰 문제점은 credit assignment 문제인데, 한 agent가 최적의 행동을 하였어도 다른 agent때문에 전체의 reward가 줄어들면, 이러한 최적의 행동을 할 확률이 줄어들게 됩니다.이런 문제를 NL에서는 신경을 못쓰기 때문에 더욱 문제가 됩니다.  
  
‌  
Chapter 3 Counterfactual Multi-Agent Poilicy Gradidents\(COMA\)  
‌  
Chapter 3에서는 Counterfactual Multi-Agent Poilicy Gradidents\(COMA\)에 대해 배울 건데, COMA는 centralised critic을 사용해서 모든 agent간의 행동에 대한 value function을 사용하기 때문에 이러한 한계점을 극복할 수 있다. 아래에 설명한 difference reward\(1\)에 착안해서 여기서는 value function을 counterfactual의 baseline을 계산하는데 사용했는데, 이 baseline은 다른 agent가 어떤 행동을 했을 때 어떤 일이 일어날지에 대한 평균에 대한 추정입니다. 이를 StarCraft micromanagement에 적용했는데, 좋은 baseline이 되었습니다.  
  
‌  
COMA는 agent들의 행동을 완전히 요소화하여 \(2\)joint value function을 배우게 됩니다. 다른말로, action이 여러 다른 agent들의 의한 joint action이 되도록 독립적으로 sampling 한다는 말입니다. 하지만 몇몇의 상황에서는 이러한 이러한 구성이 최적의 전략을 짜지 못할 수 있습니다.   
  
‌  
특히 한 agent의 최적의 행동이 다른 agent의 최적의 action을 뽑는데 독립적이지 않다면,  최적의 행동과 벗어난 행동을 하도록 만들 수 있습니다.  
  
‌  
이러한 문제는 centralised controller에 의해 해결될 수 있는데, decentralised 된 execution에는 잘 작동하지 않을 수 있습니다.  
  
‌  
Chapter 4 Multi-Agent Common Knowledge Reinforcement Learning\(MACKRL\)  
‌  
Chapter 4 에서 여기서는 Multi-Agent Common Knowledge Reinforcement Learning\(MACKRL\)를 소개합니다. MACKRL 에서는 group내에서 agent들이 서로 common knowledge를 가지게 되는데, 이는 fully decentralised 상황에서 joint action을 배우기 위함입니다. 그룹의 common knowledge라 함은 모든 agent가 알고 모든 agent가 모든 agent가 안다는 것을 아는 상태를 말합니다. 흥미로운건 MARL의 여러 환경에서 agent는 다른 agent들을 관찰하고, 그럼으로써 그들은 common knowledge 를 가지게 됩니다. 특히 MACKRL은 hierarchical controller에 의존하는데, 이 것은 그룹의 agent들에게 joint action 을 할당해주거나, 어떻게 agent들을 어떻게 분할해 협동해야하는지를 정해줍니다. 중요한 것은 각각의 세부 subgroup들은 조직화된 action을 고르는데 있어선 부족하지만 common knowledge 를 공유할 것입니다. 그러므로 MACKRL controller는 엄청난 양의 정보를 가지고 있는 개개인의 actor와 subgroup의 조직화된 action사이에 trade off를 배울 것입니다.  
  
‌  
COMA와 MACKRL은 on-policy method 이기 때문에, 현재의 policy만 가지고 학습을 하게 됩니다. 하지만, off-policy는 sample efficiency가 높습니다. 좀더 안정된 learning을 위해서, off-policy Deep RL은 replay memory를 사용하는데 초점이 맞추어져 있습니다.  
  
‌  
그 multiple learning agent는 환경을 nonstationary하게 만듭니다. state-action 에 대한 reward는 다른 agent들에 독립적이지 않습니다. 모든 agent는 그들의 policy를 update하고, reward 기댓값은 계속해서 변하게 됩니다.  
  
‌  
이러한 문제를 풀기위해서 안정화된 Replay buffer를 사용하는 새로운 방법을 소개하는데, centralised 된 학습중에 multi-agent에 importance weights 를 사용하고, metadata인 fingerprint를 넣어주는 방법입니다. fingerprint는 episode로 부터 분리시켜주고, 다른 agent들의 policy와 구분될 수 있게 해줍니다.  
  
‌  
이 Chapter들은 다음의 논문에 따라 진행됩니다.  
  
‌  
'Counterfactual Multi-Agent Policy Gradient' Jakob Foerster, et al.  
  
'Stabilising Experience replay for deep multi-agent reinforcement learning' Jakob Foerster, et al.  
  
'Multi-Agent Common-Knowledge Reinforcement Learning', Christian A Schroeder de Witt, et al.  
  
‌  
Part 2 : Learning to Communicate  
‌  
지금까지는 agent간의 명확한 communication 방법에 대해 생각해보지 않았습니다. 하지만, 실제 적용단계에서는 communication방법이 굉장히 제한되어있습니다. Chapter 6 에서는 어떻게 agent가 Cheap-talk 채널만을 사용하면서 communication protocol을 배우는지 배울 것입니다. Cheap-talk이란 environment에서 transition probability나 reward에 영향을 미치지 않는, 즉, environment와 독립적인 channel이란 뜻입니다.   
  
‌  
Differentiable Inter-Agent Learning\(DIAL\)과 Reinforced Inter-Agent Learning\(RIAL\)을 제안하는데, centralised training동안, DIAL은 continuous activation으로써  messages를 모델링하고, 어떤 message를 보내야 할지에 대해 배우기 위해 gradients를 보냅니다.  
  
‌  
하지만 RIAL은 메세지를 action space의 일부로 생각하고, 어떤 message를 보내야할지 배웁니다. Chapter 7에서는 Bayesian Action Decoder\(BAD\)를 제안하는데, 이는 RIAL의 확장된 버전입니다. DIAL와는 다르게, BAD는 cheap talk channel없이 사용될 수 있는데 특히, agent가 action을 취했을 때,  다른 agent로 부터 관찰됨으로써 communicate되도록 했습니다. 직접적으로 다른 agent를 관찰하며 그들의 상태를 추론함으로써, BAD는 Hanabi game에서 SOTA를 달성했습니다. 다음과 같은 논문들을 기반으로 쓰여졌습니다.  
  
‌  
'Learning to communicate with deep multi-agent reinforcement learning' Jakob Foerster,et al.  
  
'Bayesian Action Decoder for Deep Multi-Agent Reinforcement Learning' Jakob Foerster,et al.  
  
‌  
Part 3 : Learning to Reciprocate  
‌  
지금까지는 cooperative한 환경에서 team reward를 최대화 하기 위한 방법들에 대해 배웠습니다. 하지만, 많은 실제 문제는 개인의 목적을 최대화하는 데에 목적을 두고 있습니다. 예를 들면, 모든 driver는 자신의 목적지까지 최대한 빨리 도착하는 것을 원하지 전체 도로의 효율성을 따지지 않습니다. Game theory는 이러한 상황에서 최선의 전략을 짜는데 오랜 역사가 있습니다. 여기에는 내쉬 균형\(Nash equilibrium\)이라는 게임이론의 중점적인 개념이있는데, 어떤 agent도 자신이 policy를 바꾼다고 더 나은 return을 받지 못하는 균형상태를 말합니다. 하지만 게임이론에서 내쉬 균형은 알려져있고, 계산가능하지만, RL에서는 다릅니다. MARL에서 agent는 environment와의 상호작용에 의존합니다. 반면에 NL은 cooperative한 환경에선 놀랄만큼 잘 동작했지만, general sum issue가 발생할 수 있습니다. 첫번째로, 모든 agent는 그들의 objective를 최대화하는 것은 안정적이지 않고, 둘째로, agent는 상호 응답하는데 실패해 안좋은 상황에서 내쉬균형을 이룰 수 있습니다. 다음과 같은 이슈들은 다른 agent들을 정적인 환경으로 취급하기 때문입니다. Learning with opponent-Learning Awareness\(LOLA\)는 이러한 이슈들을 극복하고 agent들이 높은 reward를 가지는 내쉬균형에 이르도록 돕습니다. 다른 agent들이 정적이라고 가정하는 것 보다, 다른 agent들도 learner라고 가정하고 다른 agent의 예상 reward를 계산하도록 합니다. 중요한 것은 agent들은 상대방의 학습단계에서 미분가능하고, 이 것이 그들의 policy를 만드는 것을 이끕니다. LOLA agents들은 반복적인 prisoner's dilemma에서 tit-for-tat 전략을 발견하는 것을 보였습니다. 한가지 기술적인 어려움은 agent의 learning step을 미분하면 나오는 environment에서  sample을 뽑아 추정해야하는 높은 계수의 도함수이다. 이를 해결하기 위해 Chapter 9에서는 The Infinitely Differentiable Monte-Carlo Estimator\(DiCE\)를 도입했다.

* 'Learning with Opponent-Learning Awareness' Jakob N Foerster
* 'DiCE: The Infinitely Differentiable Monte-Carlo Estimator' Jakob N Foerster





### \(1\)Difference reward

reward structure를 정할때, 가장 쉬운 접근은 모든 agent가 system이 받는대로 받는 것 입니다. 하지만 이는 너무 느린 수렴을 보이고, 그래서 여기서는 agent 개별 reward를 주는 것에 착안했습니다. agent들은 그들 각자의 reward를 최대화하는 것인데, 중요한 행동을 했을 때, 높은 reward를 주어 전체적인 시스템이 좋은 performance를 보이도록 해야합니다. 그래서 여기서 difference reward에 집중하였습니다.

### \(2\)Joint

joint라는 단어가 자주 나오게 되는데 이는, 결합 확률 분포를 생각하면 쉽습니다.결합확률 분포는 확률 변수가 두가지 이상 결합되어있는 분포입니다. 다른 agent의 행동이 stochastic하므로 Joint value를 배우는 것이 정확한 표현입니다.

### reference

* Kagan Tumer and Adrian Agogino. "Distributed Agent-Based Air Traffic Flow Management"\(2007\) 

